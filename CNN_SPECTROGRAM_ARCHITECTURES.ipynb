{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlfsOApLQkb3/CCqyheB+r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c8d4e04ac5c4e8e89be2b1f4036cf85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccb44c8a70ce43f9937e0ddfcf09c64c",
              "IPY_MODEL_4366176900774ae5976e0e5c1e1aed52",
              "IPY_MODEL_9cfd165f07b1404fb63919748b941012"
            ],
            "layout": "IPY_MODEL_19c6bb40d9244265a545345d6a52f8e3"
          }
        },
        "ccb44c8a70ce43f9937e0ddfcf09c64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f3478addb854ca1a8c04f9581c0adb7",
            "placeholder": "​",
            "style": "IPY_MODEL_d355ee76980a49f2b1d8a032947ff2a2",
            "value": "model.safetensors: 100%"
          }
        },
        "4366176900774ae5976e0e5c1e1aed52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37caa8a39d0b4a43afc5f0b66ca26fbb",
            "max": 49335454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3984f65d5e9e4c9eb2f3cc1c45eb38ae",
            "value": 49335454
          }
        },
        "9cfd165f07b1404fb63919748b941012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_229173e87720455d95fb5b7e8728a204",
            "placeholder": "​",
            "style": "IPY_MODEL_2bd50c2cff2442619e06e119f1ca547a",
            "value": " 49.3M/49.3M [00:01&lt;00:00, 46.2MB/s]"
          }
        },
        "19c6bb40d9244265a545345d6a52f8e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f3478addb854ca1a8c04f9581c0adb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d355ee76980a49f2b1d8a032947ff2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37caa8a39d0b4a43afc5f0b66ca26fbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3984f65d5e9e4c9eb2f3cc1c45eb38ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "229173e87720455d95fb5b7e8728a204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd50c2cff2442619e06e119f1ca547a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjorisarabhai/OIBSIP/blob/main/CNN_SPECTROGRAM_ARCHITECTURES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import glob\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "LOCAL_ZIP_NAME = 'project_spectrogram_data.zip'\n",
        "SPECTROGRAM_FOLDER_NAME = 'spectrogram'\n",
        "LOCAL_CSV_NAME = 'consolidated_genres.csv'  # File name user uploads\n",
        "CLEANED_CSV_NAME = 'cleaned_final_metadata.csv'  # New standardized name for the output\n",
        "LOCAL_BASE_DIR = './project_data/'\n",
        "SPECTROGRAM_FILE_EXTENSION = '.jpg'\n",
        "\n",
        "# Global variables will be set after extraction\n",
        "SPECTROGRAM_DIR = None\n",
        "METADATA_FILE = None\n",
        "NUM_CLASSES = None\n",
        "train_loader = None\n",
        "val_loader = None\n",
        "genre_names = None\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- FILE UPLOAD AND AUTO-CORRECTING EXTRACTION UTILITY ---\n",
        "def upload_and_extract_data():\n",
        "    \"\"\"Prompts for file upload, extracts ZIP, and finds the true paths.\"\"\"\n",
        "    global SPECTROGRAM_DIR, METADATA_FILE\n",
        "    print(f\"Please upload your '{LOCAL_ZIP_NAME}' file now:\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"❌ ERROR: No file uploaded. Aborting setup.\")\n",
        "        return False\n",
        "    uploaded_filename = list(uploaded.keys())[0]\n",
        "    os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
        "    try:\n",
        "        with zipfile.ZipFile(uploaded_filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall(LOCAL_BASE_DIR)\n",
        "        os.remove(uploaded_filename)\n",
        "        # Find the correct, deep paths for SPECTROGRAM_DIR and METADATA_FILE\n",
        "        spectro_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', SPECTROGRAM_FOLDER_NAME), recursive=True)\n",
        "        csv_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)\n",
        "        if not spectro_paths or not csv_paths:\n",
        "            print(f\"❌ FATAL ERROR: Could not find required files inside the extracted data.\")\n",
        "            return False\n",
        "\n",
        "        # Set the global paths\n",
        "        SPECTROGRAM_DIR = spectro_paths[0]\n",
        "        # We will point METADATA_FILE to the temporary output of the cleansing script\n",
        "        METADATA_FILE = os.path.join(os.path.dirname(csv_paths[0]), CLEANED_CSV_NAME)\n",
        "        print(f\"✅ SUCCESS: Data extracted and paths located.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: Failed during extraction or path search: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- DATA CLEANSING UTILITY (Identify all label columns dynamically) ---\n",
        "def cleanse_metadata_file():\n",
        "    \"\"\"Filters the metadata CSV to use all available labels and synchronizes with physical files.\"\"\"\n",
        "    original_csv_path = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)[0]\n",
        "    metadata_df = pd.read_csv(original_csv_path)\n",
        "    print(f\"Original metadata columns: {metadata_df.columns.tolist()}\")\n",
        "    print(f\"Original metadata sample:\\n{metadata_df.head()}\")\n",
        "\n",
        "    # Sync filenames/extensions\n",
        "    metadata_df['filename'] = metadata_df['filename'].str.replace(r'\\.(png|jpeg|jpg)$', SPECTROGRAM_FILE_EXTENSION, regex=True)\n",
        "    image_files = set(os.listdir(SPECTROGRAM_DIR))\n",
        "    print(f\"Image files found: {len(image_files)}; Examples: {list(image_files)[:5]}\")\n",
        "\n",
        "    # Filter by filenames matching files\n",
        "    filtered_df = metadata_df[metadata_df['filename'].isin(image_files)].copy()\n",
        "    print(f\"Rows after filename syncing: {len(filtered_df)}\")\n",
        "\n",
        "    # Detect label columns: assume all except 'filename' are labels\n",
        "    filename_col = filtered_df.columns[0]\n",
        "    genre_columns = list(filtered_df.columns[1:])  # treat all other columns as labels\n",
        "    print(f\"Label columns detected: {genre_columns}\")\n",
        "\n",
        "    # Prepare final DataFrame\n",
        "    final_columns = [filename_col] + genre_columns\n",
        "    final_df = filtered_df[final_columns].copy()\n",
        "\n",
        "    # Keep only tracks with at least one label\n",
        "    final_df['label_sum'] = final_df[genre_columns].sum(axis=1)\n",
        "    final_df = final_df[final_df['label_sum'] > 0].drop(columns=['label_sum'])\n",
        "\n",
        "    print(f\"Rows after removing tracks with no labels: {len(final_df)}\")\n",
        "    print(f\"Final labels count: {len(genre_columns)}\")\n",
        "\n",
        "    # Save cleaned data\n",
        "    final_df.to_csv(METADATA_FILE, index=False)\n",
        "    print(f\"✅ DATA CLEANSING COMPLETE\")\n",
        "    return True\n",
        "\n",
        "# --- CUSTOM MULTI-LABEL DATASET CLASS ---\n",
        "class MultiLabelSpectrogramDataset(Dataset):\n",
        "    def __init__(self, metadata_path, img_dir, transform=None):\n",
        "        self.metadata_frame = pd.read_csv(metadata_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        # rename first column to 'filename' (for safety)\n",
        "        self.metadata_frame.rename(columns={self.metadata_frame.columns[0]: 'filename'}, inplace=True)\n",
        "        self.label_columns = self.metadata_frame.columns[1:].tolist()\n",
        "        self.num_classes = len(self.label_columns)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.metadata_frame.iloc[idx, 0]\n",
        "        img_path = os.path.join(self.img_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label_vector = self.metadata_frame.iloc[idx, 1:].values.astype(np.float32)\n",
        "        label_tensor = torch.tensor(label_vector, dtype=torch.float32)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label_tensor\n",
        "\n",
        "# --- RUN INITIALIZATION ---\n",
        "if upload_and_extract_data():\n",
        "    if cleanse_metadata_file():\n",
        "        # Data transforms\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x)\n",
        "        ])\n",
        "        val_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x)\n",
        "        ])\n",
        "\n",
        "        full_dataset = MultiLabelSpectrogramDataset(\n",
        "            metadata_path=METADATA_FILE,\n",
        "            img_dir=SPECTROGRAM_DIR\n",
        "        )\n",
        "\n",
        "        global NUM_CLASSES, train_loader, val_loader, genre_names\n",
        "        NUM_CLASSES = full_dataset.num_classes\n",
        "        train_size = int(0.8 * len(full_dataset))\n",
        "        val_size = len(full_dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "        train_dataset.dataset.transform = train_transform\n",
        "        val_dataset.dataset.transform = val_transform\n",
        "\n",
        "        batch_size = 16\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        genre_names = full_dataset.label_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "id": "rIZmZbKcIKYT",
        "outputId": "236ef9d1-cfc5-41b4-b475-0fb14253809c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Please upload your 'project_spectrogram_data.zip' file now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-62895ec5-6903-419d-a704-da225b9000a2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-62895ec5-6903-419d-a704-da225b9000a2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving project_spectrogram_data.zip to project_spectrogram_data.zip\n",
            "✅ SUCCESS: Data extracted and paths located.\n",
            "Original metadata columns: ['filename', 'Hip hop', 'Rap', 'R&B / Soul', 'Pop', 'Rock', 'Jazz', 'Blues', 'Country', 'Folk', 'Classical', 'Electronic', 'Funk', 'Reggae / Ska / Dub', 'Metal', 'Gospel / Religious', 'Latin', 'Disco / Dance', 'Ambient / Chill / New Age', 'Experimental / Avant-Garde', 'Opera / Musical Theater / Soundtrack', 'Vocal / A cappella', 'Dancehall / Hip House / Club', 'Psychedelic', 'Other / Niche']\n",
            "Original metadata sample:\n",
            "                                   filename  Hip hop  Rap  R&B / Soul  Pop  \\\n",
            "0  00b1397d-7f3e-4c59-bb42-ccd7fa17ee10.jpg        0    0           0    1   \n",
            "1  00c9dcab-4abf-47f5-9755-c5c805b779c7.jpg        1    1           1    0   \n",
            "2  012e3459-b54d-49e9-b48d-d0922d295c5a.jpg        0    0           0    1   \n",
            "3  013a7fe3-0113-4604-a295-f74a0b88bf05.jpg        0    0           0    0   \n",
            "4  0172efb9-b353-4e55-82cd-80136d98069f.jpg        0    0           0    0   \n",
            "\n",
            "   Rock  Jazz  Blues  Country  Folk  ...  Gospel / Religious  Latin  \\\n",
            "0     0     0      0        0     0  ...                   0      0   \n",
            "1     0     0      0        0     0  ...                   0      0   \n",
            "2     1     0      0        0     0  ...                   0      0   \n",
            "3     1     0      0        0     0  ...                   0      0   \n",
            "4     1     0      0        0     0  ...                   0      0   \n",
            "\n",
            "   Disco / Dance  Ambient / Chill / New Age  Experimental / Avant-Garde  \\\n",
            "0              0                          0                           0   \n",
            "1              0                          0                           0   \n",
            "2              0                          0                           0   \n",
            "3              0                          0                           0   \n",
            "4              0                          0                           0   \n",
            "\n",
            "   Opera / Musical Theater / Soundtrack  Vocal / A cappella  \\\n",
            "0                                     0                   1   \n",
            "1                                     0                   0   \n",
            "2                                     0                   0   \n",
            "3                                     0                   0   \n",
            "4                                     0                   0   \n",
            "\n",
            "   Dancehall / Hip House / Club  Psychedelic  Other / Niche  \n",
            "0                             0            0              0  \n",
            "1                             0            0              0  \n",
            "2                             0            0              0  \n",
            "3                             0            0              0  \n",
            "4                             0            0              0  \n",
            "\n",
            "[5 rows x 25 columns]\n",
            "Image files found: 1083; Examples: ['b0d8029f-9326-40bd-a98d-e9700a7223e6.jpg', '591cd20b-d903-4e32-97b5-1ec8519a8ac3.jpg', 'd8ccf243-a78b-4397-b331-18da5ecc5759.jpg', 'ad90862b-80df-4eb5-8ace-5c89b9ad6ce3.jpg', '6d1a627a-05e1-4696-b3d2-c5575f45d72b.jpg']\n",
            "Rows after filename syncing: 1033\n",
            "Label columns detected: ['Hip hop', 'Rap', 'R&B / Soul', 'Pop', 'Rock', 'Jazz', 'Blues', 'Country', 'Folk', 'Classical', 'Electronic', 'Funk', 'Reggae / Ska / Dub', 'Metal', 'Gospel / Religious', 'Latin', 'Disco / Dance', 'Ambient / Chill / New Age', 'Experimental / Avant-Garde', 'Opera / Musical Theater / Soundtrack', 'Vocal / A cappella', 'Dancehall / Hip House / Club', 'Psychedelic', 'Other / Niche']\n",
            "Rows after removing tracks with no labels: 980\n",
            "Final labels count: 24\n",
            "✅ DATA CLEANSING COMPLETE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16"
      ],
      "metadata": {
        "id": "1EbaTvhpP_rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- OPTIMIZED TRAINING FUNCTION (VGG TRANSFER LEARNING) ---\n",
        "\n",
        "def train_and_evaluate_transfer_learning(model_name, model, train_loader, val_loader, NUM_CLASSES, genre_names, device):\n",
        "    \"\"\"\n",
        "    Two-phase fine-tuning (VGG Transfer Learning) for Multi-Label Classification.\n",
        "    Fixed device mismatch error and added epoch-wise progress.\n",
        "    \"\"\"\n",
        "    # --- Phase 0: Adapt final layer & move to device\n",
        "    if model_name.startswith('VGG'):\n",
        "        num_ftrs = model.classifier[6].in_features\n",
        "        model.classifier[6] = nn.Linear(num_ftrs, NUM_CLASSES).to(device)  # <-- FIXED DEVICE\n",
        "\n",
        "    model.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # --- Phase 1: Train classifier head only ---\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "    num_epochs_phase1 = 5\n",
        "\n",
        "    print(f\"\\n--- Phase 1: Training {model_name} Classifier Head (5 Epochs) ---\")\n",
        "    for epoch in range(num_epochs_phase1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Phase 1 - Epoch {epoch+1}/{num_epochs_phase1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # --- Phase 2: Fine-tune last layers ---\n",
        "    for param in model.features[24:].parameters():\n",
        "        param.requires_grad = True\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "    num_epochs_phase2 = 40\n",
        "    print(\"\\nPhase 2: Fine-tuning last layers (40 Epochs)...\")\n",
        "    for epoch in range(num_epochs_phase2):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Phase 2 - Epoch {epoch+1}/{num_epochs_phase2}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # --- FINAL EVALUATION ---\n",
        "    model.eval()\n",
        "    y_true_list, y_pred_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted_probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "            predicted_labels = (predicted_probs > 0.5).astype(int)\n",
        "            y_true_list.extend(labels.cpu().numpy())\n",
        "            y_pred_list.extend(predicted_labels)\n",
        "\n",
        "    y_true = np.array(y_true_list)\n",
        "    y_pred = np.array(y_pred_list)\n",
        "\n",
        "    exact_match_accuracy = (y_pred == y_true).all(axis=1).mean()\n",
        "    from sklearn.metrics import f1_score, classification_report\n",
        "    weighted_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"\\n--- Final Evaluation for {model_name} ---\")\n",
        "    print(f\"Overall Weighted F1-score: {weighted_f1:.4f}\")\n",
        "    print(f\"Exact Match Accuracy: {exact_match_accuracy:.4f}\")\n",
        "    print(classification_report(y_true, y_pred, target_names=genre_names, zero_division=0))\n",
        "\n",
        "\n",
        "# --- EXECUTION: VGG-16 TRANSFER LEARNING ---\n",
        "\n",
        "# Define VGG-16 model with pre-trained weights\n",
        "vgg16_model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Run VGG-16 (Optimized for 50%+ F1-score)\n",
        "train_and_evaluate_transfer_learning(\n",
        "    model_name=\"VGG-16\",\n",
        "    model=vgg16_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    NUM_CLASSES=NUM_CLASSES,\n",
        "    genre_names=genre_names,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "p0dgtcG2RR5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c6b44d-4528-4daf-8a23-3b6b452c5666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:05<00:00, 97.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Phase 1: Training VGG-16 Classifier Head (5 Epochs) ---\n",
            "Phase 1 - Epoch 1/5, Loss: 0.2862\n",
            "Phase 1 - Epoch 2/5, Loss: 0.2420\n",
            "Phase 1 - Epoch 3/5, Loss: 0.2259\n",
            "Phase 1 - Epoch 4/5, Loss: 0.2097\n",
            "Phase 1 - Epoch 5/5, Loss: 0.2065\n",
            "\n",
            "Phase 2: Fine-tuning last layers (40 Epochs)...\n",
            "Phase 2 - Epoch 1/40, Loss: 0.1916\n",
            "Phase 2 - Epoch 2/40, Loss: 0.1795\n",
            "Phase 2 - Epoch 3/40, Loss: 0.1758\n",
            "Phase 2 - Epoch 4/40, Loss: 0.1689\n",
            "Phase 2 - Epoch 5/40, Loss: 0.1650\n",
            "Phase 2 - Epoch 6/40, Loss: 0.1619\n",
            "Phase 2 - Epoch 7/40, Loss: 0.1556\n",
            "Phase 2 - Epoch 8/40, Loss: 0.1580\n",
            "Phase 2 - Epoch 9/40, Loss: 0.1523\n",
            "Phase 2 - Epoch 10/40, Loss: 0.1453\n",
            "Phase 2 - Epoch 11/40, Loss: 0.1443\n",
            "Phase 2 - Epoch 12/40, Loss: 0.1384\n",
            "Phase 2 - Epoch 13/40, Loss: 0.1383\n",
            "Phase 2 - Epoch 14/40, Loss: 0.1363\n",
            "Phase 2 - Epoch 15/40, Loss: 0.1344\n",
            "Phase 2 - Epoch 16/40, Loss: 0.1306\n",
            "Phase 2 - Epoch 17/40, Loss: 0.1283\n",
            "Phase 2 - Epoch 18/40, Loss: 0.1270\n",
            "Phase 2 - Epoch 19/40, Loss: 0.1204\n",
            "Phase 2 - Epoch 20/40, Loss: 0.1208\n",
            "Phase 2 - Epoch 21/40, Loss: 0.1195\n",
            "Phase 2 - Epoch 22/40, Loss: 0.1131\n",
            "Phase 2 - Epoch 23/40, Loss: 0.1110\n",
            "Phase 2 - Epoch 24/40, Loss: 0.1098\n",
            "Phase 2 - Epoch 25/40, Loss: 0.1070\n",
            "Phase 2 - Epoch 26/40, Loss: 0.1045\n",
            "Phase 2 - Epoch 27/40, Loss: 0.1027\n",
            "Phase 2 - Epoch 28/40, Loss: 0.1015\n",
            "Phase 2 - Epoch 29/40, Loss: 0.0984\n",
            "Phase 2 - Epoch 30/40, Loss: 0.0965\n",
            "Phase 2 - Epoch 31/40, Loss: 0.0927\n",
            "Phase 2 - Epoch 32/40, Loss: 0.0901\n",
            "Phase 2 - Epoch 33/40, Loss: 0.0858\n",
            "Phase 2 - Epoch 34/40, Loss: 0.0857\n",
            "Phase 2 - Epoch 35/40, Loss: 0.0825\n",
            "Phase 2 - Epoch 36/40, Loss: 0.0803\n",
            "Phase 2 - Epoch 37/40, Loss: 0.0780\n",
            "Phase 2 - Epoch 38/40, Loss: 0.0759\n",
            "Phase 2 - Epoch 39/40, Loss: 0.0743\n",
            "Phase 2 - Epoch 40/40, Loss: 0.0717\n",
            "\n",
            "--- Final Evaluation for VGG-16 ---\n",
            "Overall Weighted F1-score: 0.5175\n",
            "Exact Match Accuracy: 0.2092\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                             Hip hop       0.73      0.42      0.53        38\n",
            "                                 Rap       0.65      0.38      0.48        34\n",
            "                          R&B / Soul       0.55      0.49      0.52        45\n",
            "                                 Pop       0.73      0.86      0.79       109\n",
            "                                Rock       0.67      0.53      0.59        57\n",
            "                                Jazz       0.00      0.00      0.00         7\n",
            "                               Blues       0.00      0.00      0.00         1\n",
            "                             Country       0.54      0.61      0.58        31\n",
            "                                Folk       0.56      0.33      0.42        15\n",
            "                           Classical       0.00      0.00      0.00         3\n",
            "                          Electronic       0.35      0.48      0.41        33\n",
            "                                Funk       0.00      0.00      0.00         1\n",
            "                  Reggae / Ska / Dub       0.00      0.00      0.00         0\n",
            "                               Metal       0.50      1.00      0.67         2\n",
            "                  Gospel / Religious       0.00      0.00      0.00         1\n",
            "                               Latin       0.00      0.00      0.00         1\n",
            "                       Disco / Dance       0.33      0.50      0.40        20\n",
            "           Ambient / Chill / New Age       0.18      0.40      0.25         5\n",
            "          Experimental / Avant-Garde       0.00      0.00      0.00        13\n",
            "Opera / Musical Theater / Soundtrack       0.00      0.00      0.00         2\n",
            "                  Vocal / A cappella       0.60      0.46      0.52        26\n",
            "        Dancehall / Hip House / Club       0.00      0.00      0.00         5\n",
            "                         Psychedelic       0.00      0.00      0.00        10\n",
            "                       Other / Niche       0.00      0.00      0.00         7\n",
            "\n",
            "                           micro avg       0.58      0.52      0.55       466\n",
            "                           macro avg       0.27      0.27      0.26       466\n",
            "                        weighted avg       0.54      0.52      0.52       466\n",
            "                         samples avg       0.56      0.56      0.53       466\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "LOCAL_ZIP_NAME = 'project_spectrogram_data.zip'\n",
        "LOCAL_BASE_DIR = './project_data/'\n",
        "SPECTROGRAM_FOLDER_NAME = 'spectrogram'\n",
        "LOCAL_CSV_NAME = 'consolidated_genres.csv'\n",
        "CLEANED_CSV_NAME = 'cleaned_final_metadata.csv'\n",
        "SPECTROGRAM_FILE_EXTENSION = '.jpg'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_PHASE1 = 5\n",
        "EPOCHS_PHASE2 = 50\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "SPECTROGRAM_DIR = None\n",
        "METADATA_FILE = None\n",
        "\n",
        "# --- UPLOAD AND EXTRACT ZIP ---\n",
        "def upload_and_extract_data():\n",
        "    global SPECTROGRAM_DIR, METADATA_FILE\n",
        "    print(f\"Please upload your '{LOCAL_ZIP_NAME}' file now:\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"❌ No file uploaded. Aborting.\")\n",
        "        return False\n",
        "    zip_filename = list(uploaded.keys())[0]\n",
        "    os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_BASE_DIR)\n",
        "    os.remove(zip_filename)\n",
        "\n",
        "    # Locate spectrogram folder and CSV file\n",
        "    spectro_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', SPECTROGRAM_FOLDER_NAME), recursive=True)\n",
        "    csv_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)\n",
        "    if not spectro_paths or not csv_paths:\n",
        "        print(\"❌ Could not find spectrogram folder or CSV in extracted data.\")\n",
        "        return False\n",
        "\n",
        "    SPECTROGRAM_DIR = spectro_paths[0]\n",
        "    METADATA_FILE = os.path.join(os.path.dirname(csv_paths[0]), CLEANED_CSV_NAME)\n",
        "    print(f\"✅ Extraction complete. Spectrogram dir: {SPECTROGRAM_DIR}\")\n",
        "    return True\n",
        "\n",
        "# --- DATA CLEANSING ---\n",
        "def cleanse_metadata_file():\n",
        "    original_csv_path = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)[0]\n",
        "    metadata_df = pd.read_csv(original_csv_path)\n",
        "\n",
        "    # Standardize filename extensions for syncing\n",
        "    metadata_df['filename'] = metadata_df['filename'].str.replace(r'\\.(png|jpeg|jpg)$',\n",
        "                                                                  SPECTROGRAM_FILE_EXTENSION,\n",
        "                                                                  regex=True)\n",
        "    image_files = set(os.listdir(SPECTROGRAM_DIR))\n",
        "    metadata_df = metadata_df[metadata_df['filename'].isin(image_files)].copy()\n",
        "\n",
        "    filename_col = metadata_df.columns[0]\n",
        "    label_cols = list(metadata_df.columns[1:])\n",
        "\n",
        "    final_columns = [filename_col] + label_cols\n",
        "    final_df = metadata_df[final_columns].copy()\n",
        "\n",
        "    # Keep only entries with at least one label\n",
        "    final_df['label_sum'] = final_df[label_cols].sum(axis=1)\n",
        "    final_df = final_df[final_df['label_sum'] > 0].drop(columns=['label_sum'])\n",
        "\n",
        "    final_df.to_csv(METADATA_FILE, index=False)\n",
        "    print(f\"✅ Data cleansing complete with {len(final_df)} samples and {len(label_cols)} labels.\")\n",
        "    return label_cols\n",
        "\n",
        "# --- DATASET DEFINITION ---\n",
        "class MultiLabelSpectrogramDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self.df.columns[1:].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label_vec = self.df.iloc[idx, 1:].values.astype(np.float32)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(label_vec, dtype=torch.float32)\n",
        "\n",
        "# --- DATA AUGMENTATIONS ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x)\n",
        "])\n",
        "\n",
        "# --- TRAINING FUNCTION ---\n",
        "def train_model():\n",
        "    if not upload_and_extract_data():\n",
        "        return\n",
        "    label_cols = cleanse_metadata_file()\n",
        "\n",
        "    dataset = MultiLabelSpectrogramDataset(METADATA_FILE, SPECTROGRAM_DIR, transform=None)\n",
        "    dataset_size = len(dataset)\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    val_size = dataset_size - train_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "    train_ds.dataset.transform = train_transform\n",
        "    val_ds.dataset.transform = val_transform\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, len(label_cols))\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    base_params = [p for n, p in model.named_parameters() if 'classifier' not in n]\n",
        "    head_params = [p for n, p in model.named_parameters() if 'classifier' in n]\n",
        "\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': base_params, 'lr': 1e-5},\n",
        "        {'params': head_params, 'lr': 1e-3}\n",
        "    ], weight_decay=1e-4)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE2)\n",
        "\n",
        "    # Phase 1: Train head\n",
        "    print(\"--- Phase 1: Training classifier head ---\")\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS_PHASE1):\n",
        "        total_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS_PHASE1} Loss: {total_loss / len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Phase 2: Fine-tuning entire model\n",
        "    print(\"\\n--- Phase 2: Fine-tuning entire model ---\")\n",
        "    for epoch in range(EPOCHS_PHASE2):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS_PHASE2} Loss: {total_loss / len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Evaluation on validation set\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_targets.append(targets.numpy())\n",
        "\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    pred_labels = (all_preds > 0.5).astype(int)\n",
        "    weighted_f1 = f1_score(all_targets, pred_labels, average='weighted', zero_division=0)\n",
        "    print(f\"\\nFinal Weighted F1 score on validation data: {weighted_f1:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "id": "m41GtPthRett",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "721327aa-b050-4381-d12c-577e7a3920af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Please upload your 'project_spectrogram_data.zip' file now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-58fad83b-b8b5-4784-920a-55b4fd804cba\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-58fad83b-b8b5-4784-920a-55b4fd804cba\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving project_spectrogram_data.zip to project_spectrogram_data.zip\n",
            "✅ Extraction complete. Spectrogram dir: ./project_data/project_spectrogram_data/spectrogram\n",
            "✅ Data cleansing complete with 980 samples and 24 labels.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Training classifier head ---\n",
            "Epoch 1/5 Loss: 0.2971\n",
            "Epoch 2/5 Loss: 0.2235\n",
            "Epoch 3/5 Loss: 0.1830\n",
            "Epoch 4/5 Loss: 0.1433\n",
            "Epoch 5/5 Loss: 0.1051\n",
            "\n",
            "--- Phase 2: Fine-tuning entire model ---\n",
            "Epoch 1/50 Loss: 0.0000\n",
            "Epoch 2/50 Loss: 0.0000\n",
            "Epoch 3/50 Loss: 0.0000\n",
            "Epoch 4/50 Loss: 0.0000\n",
            "Epoch 5/50 Loss: 0.0000\n",
            "Epoch 6/50 Loss: 0.0000\n",
            "Epoch 7/50 Loss: 0.0000\n",
            "Epoch 8/50 Loss: 0.0000\n",
            "Epoch 9/50 Loss: 0.0000\n",
            "Epoch 10/50 Loss: 0.0000\n",
            "Epoch 11/50 Loss: 0.0000\n",
            "Epoch 12/50 Loss: 0.0000\n",
            "Epoch 13/50 Loss: 0.0000\n",
            "Epoch 14/50 Loss: 0.0000\n",
            "Epoch 15/50 Loss: 0.0000\n",
            "Epoch 16/50 Loss: 0.0000\n",
            "Epoch 17/50 Loss: 0.0000\n",
            "Epoch 18/50 Loss: 0.0000\n",
            "Epoch 19/50 Loss: 0.0000\n",
            "Epoch 20/50 Loss: 0.0000\n",
            "Epoch 21/50 Loss: 0.0000\n",
            "Epoch 22/50 Loss: 0.0000\n",
            "Epoch 23/50 Loss: 0.0000\n",
            "Epoch 24/50 Loss: 0.0000\n",
            "Epoch 25/50 Loss: 0.0000\n",
            "Epoch 26/50 Loss: 0.0000\n",
            "Epoch 27/50 Loss: 0.0000\n",
            "Epoch 28/50 Loss: 0.0000\n",
            "Epoch 29/50 Loss: 0.0000\n",
            "Epoch 30/50 Loss: 0.0000\n",
            "Epoch 31/50 Loss: 0.0000\n",
            "Epoch 32/50 Loss: 0.0000\n",
            "Epoch 33/50 Loss: 0.0000\n",
            "Epoch 34/50 Loss: 0.0000\n",
            "Epoch 35/50 Loss: 0.0000\n",
            "Epoch 36/50 Loss: 0.0000\n",
            "Epoch 37/50 Loss: 0.0000\n",
            "Epoch 38/50 Loss: 0.0000\n",
            "Epoch 39/50 Loss: 0.0000\n",
            "Epoch 40/50 Loss: 0.0000\n",
            "Epoch 41/50 Loss: 0.0000\n",
            "Epoch 42/50 Loss: 0.0000\n",
            "Epoch 43/50 Loss: 0.0000\n",
            "Epoch 44/50 Loss: 0.0000\n",
            "Epoch 45/50 Loss: 0.0000\n",
            "Epoch 46/50 Loss: 0.0000\n",
            "Epoch 47/50 Loss: 0.0000\n",
            "Epoch 48/50 Loss: 0.0000\n",
            "Epoch 49/50 Loss: 0.0000\n",
            "Epoch 50/50 Loss: 0.0000\n",
            "\n",
            "Final Weighted F1 score on validation data: 0.5173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "# --- CONFIG ---\n",
        "LOCAL_ZIP_NAME = 'project_spectrogram_data.zip'\n",
        "LOCAL_BASE_DIR = './project_data/'\n",
        "SPECTROGRAM_FOLDER_NAME = 'spectrogram'\n",
        "LOCAL_CSV_NAME = 'consolidated_genres.csv'\n",
        "CLEANED_CSV_NAME = 'cleaned_final_metadata.csv'\n",
        "SPECTROGRAM_FILE_EXTENSION = '.jpg'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_PHASE1 = 5\n",
        "EPOCHS_PHASE2 = 50\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "SPECTROGRAM_DIR = None\n",
        "METADATA_FILE = None\n",
        "\n",
        "# --- UPLOAD AND EXTRACT ---\n",
        "def upload_and_extract_data():\n",
        "    global SPECTROGRAM_DIR, METADATA_FILE\n",
        "    print(f\"Please upload your '{LOCAL_ZIP_NAME}' file now:\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"❌ No file uploaded. Aborting.\")\n",
        "        return False\n",
        "    zip_filename = list(uploaded.keys())[0]\n",
        "    os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_BASE_DIR)\n",
        "    os.remove(zip_filename)\n",
        "\n",
        "    spectro_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', SPECTROGRAM_FOLDER_NAME), recursive=True)\n",
        "    csv_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)\n",
        "    if not spectro_paths or not csv_paths:\n",
        "        print(\"❌ Could not find spectrogram folder or CSV.\")\n",
        "        return False\n",
        "    SPECTROGRAM_DIR = spectro_paths[0]\n",
        "    METADATA_FILE = os.path.join(os.path.dirname(csv_paths[0]), CLEANED_CSV_NAME)\n",
        "    print(f\"✅ Extraction complete. Spectrogram dir: {SPECTROGRAM_DIR}\")\n",
        "    return True\n",
        "\n",
        "# --- DATA CLEANSING ---\n",
        "def cleanse_metadata_file():\n",
        "    original_csv_path = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)[0]\n",
        "    metadata_df = pd.read_csv(original_csv_path)\n",
        "    metadata_df['filename'] = metadata_df['filename'].str.replace(r'\\.(png|jpeg|jpg)$', SPECTROGRAM_FILE_EXTENSION, regex=True)\n",
        "    image_files = set(os.listdir(SPECTROGRAM_DIR))\n",
        "    metadata_df = metadata_df[metadata_df['filename'].isin(image_files)].copy()\n",
        "\n",
        "    filename_col = metadata_df.columns[0]\n",
        "    label_cols = list(metadata_df.columns[1:])\n",
        "    final_columns = [filename_col] + label_cols\n",
        "    final_df = metadata_df[final_columns].copy()\n",
        "\n",
        "    # Keep tracks with at least one label\n",
        "    final_df['label_sum'] = final_df[label_cols].sum(axis=1)\n",
        "    final_df = final_df[final_df['label_sum'] > 0].drop(columns=['label_sum'])\n",
        "\n",
        "    final_df.to_csv(METADATA_FILE, index=False)\n",
        "    print(f\"✅ Data cleansing complete with {len(final_df)} samples.\")\n",
        "    return label_cols\n",
        "\n",
        "# --- Dataset ---\n",
        "class MultiLabelSpectrogramDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self.df.columns[1:].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.df.iloc[idx, 0]\n",
        "        img_path = os.path.join(self.img_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label_vec = self.df.iloc[idx, 1:].values.astype(np.float32)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(label_vec, dtype=torch.float32)\n",
        "\n",
        "# --- Data Augmentations ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x)\n",
        "])\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_model():\n",
        "    if not upload_and_extract_data():\n",
        "        return\n",
        "    label_cols = cleanse_metadata_file()\n",
        "\n",
        "    dataset = MultiLabelSpectrogramDataset(METADATA_FILE, SPECTROGRAM_DIR, transform=None)\n",
        "    dataset_size = len(dataset)\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    val_size = dataset_size - train_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "    train_ds.dataset.transform = train_transform\n",
        "    val_ds.dataset.transform = val_transform\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Load VGG16\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, len(label_cols))\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    base_params = [p for n, p in model.named_parameters() if 'classifier' not in n]\n",
        "    head_params = [p for n, p in model.named_parameters() if 'classifier' in n]\n",
        "\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': base_params, 'lr': 1e-5},\n",
        "        {'params': head_params, 'lr': 1e-3}\n",
        "    ], weight_decay=1e-4)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE2)\n",
        "\n",
        "    # Phase 1\n",
        "    print(\"--- Phase 1: Training classifier head ---\")\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS_PHASE1):\n",
        "        total_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS_PHASE1} Loss: {total_loss / len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Phase 2\n",
        "    print(\"\\n--- Phase 2: Fine-tuning entire model ---\")\n",
        "    for epoch in range(EPOCHS_PHASE2):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS_PHASE2} Loss: {total_loss / len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_targets.append(targets.numpy())\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    pred_labels = (all_preds > 0.5).astype(int)\n",
        "    f1 = f1_score(all_targets, pred_labels, average='weighted', zero_division=0)\n",
        "    print(f\"\\nFinal Weighted F1 score on validation data: {f1:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "71LBW--WnjLH",
        "outputId": "2bfb17ac-bfc6-4eea-f89e-0d64b213d351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Please upload your 'project_spectrogram_data.zip' file now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3ade7dcf-327c-410f-b2b9-844cc1955042\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3ade7dcf-327c-410f-b2b9-844cc1955042\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving project_spectrogram_data.zip to project_spectrogram_data.zip\n",
            "✅ Extraction complete. Spectrogram dir: ./project_data/project_spectrogram_data/spectrogram\n",
            "✅ Data cleansing complete with 980 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Training classifier head ---\n",
            "Epoch 1/5 Loss: 0.2885\n",
            "Epoch 2/5 Loss: 0.2213\n",
            "Epoch 3/5 Loss: 0.1784\n",
            "Epoch 4/5 Loss: 0.1358\n",
            "Epoch 5/5 Loss: 0.0942\n",
            "\n",
            "--- Phase 2: Fine-tuning entire model ---\n",
            "Epoch 1/50 Loss: 0.0697\n",
            "Epoch 2/50 Loss: 0.0537\n",
            "Epoch 3/50 Loss: 0.0409\n",
            "Epoch 4/50 Loss: 0.0305\n",
            "Epoch 5/50 Loss: 0.0221\n",
            "Epoch 6/50 Loss: 0.0195\n",
            "Epoch 7/50 Loss: 0.0147\n",
            "Epoch 8/50 Loss: 0.0144\n",
            "Epoch 9/50 Loss: 0.0136\n",
            "Epoch 10/50 Loss: 0.0111\n",
            "Epoch 11/50 Loss: 0.0098\n",
            "Epoch 12/50 Loss: 0.0071\n",
            "Epoch 13/50 Loss: 0.0074\n",
            "Epoch 14/50 Loss: 0.0072\n",
            "Epoch 15/50 Loss: 0.0051\n",
            "Epoch 16/50 Loss: 0.0045\n",
            "Epoch 17/50 Loss: 0.0051\n",
            "Epoch 18/50 Loss: 0.0036\n",
            "Epoch 19/50 Loss: 0.0057\n",
            "Epoch 20/50 Loss: 0.0043\n",
            "Epoch 21/50 Loss: 0.0048\n",
            "Epoch 22/50 Loss: 0.0034\n",
            "Epoch 23/50 Loss: 0.0021\n",
            "Epoch 24/50 Loss: 0.0020\n",
            "Epoch 25/50 Loss: 0.0023\n",
            "Epoch 26/50 Loss: 0.0011\n",
            "Epoch 27/50 Loss: 0.0020\n",
            "Epoch 28/50 Loss: 0.0019\n",
            "Epoch 29/50 Loss: 0.0028\n",
            "Epoch 30/50 Loss: 0.0021\n",
            "Epoch 31/50 Loss: 0.0011\n",
            "Epoch 32/50 Loss: 0.0011\n",
            "Epoch 33/50 Loss: 0.0008\n",
            "Epoch 34/50 Loss: 0.0014\n",
            "Epoch 35/50 Loss: 0.0009\n",
            "Epoch 36/50 Loss: 0.0008\n",
            "Epoch 37/50 Loss: 0.0006\n",
            "Epoch 38/50 Loss: 0.0010\n",
            "Epoch 39/50 Loss: 0.0011\n",
            "Epoch 40/50 Loss: 0.0009\n",
            "Epoch 41/50 Loss: 0.0008\n",
            "Epoch 42/50 Loss: 0.0007\n",
            "Epoch 43/50 Loss: 0.0007\n",
            "Epoch 44/50 Loss: 0.0004\n",
            "Epoch 45/50 Loss: 0.0009\n",
            "Epoch 46/50 Loss: 0.0005\n",
            "Epoch 47/50 Loss: 0.0012\n",
            "Epoch 48/50 Loss: 0.0010\n",
            "Epoch 49/50 Loss: 0.0004\n",
            "Epoch 50/50 Loss: 0.0008\n",
            "\n",
            "Final Weighted F1 score on validation data: 0.5540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "# --- CONFIG ---\n",
        "LOCAL_ZIP_NAME = 'project_spectrogram_data.zip'\n",
        "LOCAL_BASE_DIR = './project_data/'\n",
        "SPECTROGRAM_FOLDER_NAME = 'spectrogram'\n",
        "LOCAL_CSV_NAME = 'consolidated_genres.csv'\n",
        "CLEANED_CSV_NAME = 'cleaned_final_metadata.csv'\n",
        "SPECTROGRAM_FILE_EXTENSION = '.jpg'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_PHASE1 = 5\n",
        "EPOCHS_PHASE2 = 50\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "SPECTROGRAM_DIR = None\n",
        "METADATA_FILE = None\n",
        "\n",
        "# --- UPLOAD AND EXTRACT ---\n",
        "def upload_and_extract_data():\n",
        "    global SPECTROGRAM_DIR, METADATA_FILE\n",
        "    print(f\"Please upload your '{LOCAL_ZIP_NAME}' file now:\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"❌ No file uploaded. Aborting.\")\n",
        "        return False\n",
        "    zip_filename = list(uploaded.keys())[0]\n",
        "    os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_BASE_DIR)\n",
        "    os.remove(zip_filename)\n",
        "\n",
        "    spectro_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', SPECTROGRAM_FOLDER_NAME), recursive=True)\n",
        "    csv_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)\n",
        "    if not spectro_paths or not csv_paths:\n",
        "        print(\"❌ Could not find spectrogram folder or CSV.\")\n",
        "        return False\n",
        "    SPECTROGRAM_DIR = spectro_paths[0]\n",
        "    METADATA_FILE = os.path.join(os.path.dirname(csv_paths[0]), CLEANED_CSV_NAME)\n",
        "    print(f\"✅ Extraction complete. Spectrogram dir: {SPECTROGRAM_DIR}\")\n",
        "    return True\n",
        "\n",
        "# --- DATA CLEANSING ---\n",
        "def cleanse_metadata_file():\n",
        "    original_csv_path = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)[0]\n",
        "    metadata_df = pd.read_csv(original_csv_path)\n",
        "    metadata_df['filename'] = metadata_df['filename'].str.replace(r'\\.(png|jpeg|jpg)$', SPECTROGRAM_FILE_EXTENSION, regex=True)\n",
        "    image_files = set(os.listdir(SPECTROGRAM_DIR))\n",
        "    metadata_df = metadata_df[metadata_df['filename'].isin(image_files)].copy()\n",
        "\n",
        "    filename_col = metadata_df.columns[0]\n",
        "    label_cols = list(metadata_df.columns[1:])\n",
        "    final_columns = [filename_col] + label_cols\n",
        "    final_df = metadata_df[final_columns].copy()\n",
        "\n",
        "    final_df['label_sum'] = final_df[label_cols].sum(axis=1)\n",
        "    final_df = final_df[final_df['label_sum'] > 0].drop(columns=['label_sum'])\n",
        "\n",
        "    final_df.to_csv(METADATA_FILE, index=False)\n",
        "    print(f\"✅ Data cleansing complete with {len(final_df)} samples.\")\n",
        "    return label_cols\n",
        "\n",
        "# --- Dataset ---\n",
        "class MultiLabelSpectrogramDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self.df.columns[1:].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.df.iloc[idx, 0]\n",
        "        img_path = os.path.join(self.img_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label_vec = self.df.iloc[idx, 1:].values.astype(np.float32)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(label_vec, dtype=torch.float32)\n",
        "\n",
        "# --- Data Augmentations ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x)\n",
        "])\n",
        "\n",
        "# --- TRAINING AND EVALUATION ---\n",
        "def train_and_evaluate():\n",
        "    if not upload_and_extract_data():\n",
        "        return\n",
        "    label_cols = cleanse_metadata_file()\n",
        "\n",
        "    dataset = MultiLabelSpectrogramDataset(METADATA_FILE, SPECTROGRAM_DIR, transform=None)\n",
        "    dataset_size = len(dataset)\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    val_size = dataset_size - train_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "    train_ds.dataset.transform = train_transform\n",
        "    val_ds.dataset.transform = val_transform\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Load VGG16\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, len(label_cols))\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    base_params = [p for n, p in model.named_parameters() if 'classifier' not in n]\n",
        "    head_params = [p for n, p in model.named_parameters() if 'classifier' in n]\n",
        "\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': base_params, 'lr': 1e-5},\n",
        "        {'params': head_params, 'lr': 1e-3}\n",
        "    ], weight_decay=1e-4)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE2)\n",
        "\n",
        "    # --- Phase 1: Train classifier head ---\n",
        "    print(\"--- Phase 1: Training classifier head ---\")\n",
        "    for epoch in range(EPOCHS_PHASE1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS_PHASE1} Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # --- Phase 2: Fine-tune entire model ---\n",
        "    print(\"\\n--- Phase 2: Fine-tuning entire model ---\")\n",
        "    for epoch in range(EPOCHS_PHASE2):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS_PHASE2} Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_targets.append(targets.numpy())\n",
        "\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    pred_labels = (all_preds > 0.5).astype(int)\n",
        "\n",
        "    # Overall exact-match accuracy (all labels must match)\n",
        "    overall_accuracy = np.mean(np.all(pred_labels == all_targets, axis=1))\n",
        "    print(f\"\\nOverall Exact-Match Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "    # Overall weighted metrics\n",
        "    weighted_f1 = f1_score(all_targets, pred_labels, average='weighted', zero_division=0)\n",
        "    weighted_precision = precision_score(all_targets, pred_labels, average='weighted', zero_division=0)\n",
        "    weighted_recall = recall_score(all_targets, pred_labels, average='weighted', zero_division=0)\n",
        "    print(f\"\\nOverall Weighted Metrics:\")\n",
        "    print(f\"F1 Score: {weighted_f1:.4f}\")\n",
        "    print(f\"Precision: {weighted_precision:.4f}\")\n",
        "    print(f\"Recall: {weighted_recall:.4f}\")\n",
        "\n",
        "    # Per-label metrics\n",
        "    metrics_list = []\n",
        "    label_names = dataset.labels\n",
        "    for i, label in enumerate(label_names):\n",
        "        acc = accuracy_score(all_targets[:, i], pred_labels[:, i])\n",
        "        prec = precision_score(all_targets[:, i], pred_labels[:, i], zero_division=0)\n",
        "        rec = recall_score(all_targets[:, i], pred_labels[:, i], zero_division=0)\n",
        "        f1 = f1_score(all_targets[:, i], pred_labels[:, i], zero_division=0)\n",
        "        metrics_list.append([label, acc, prec, rec, f1])\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics_list, columns=['Label', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
        "    print(\"\\nPer-label Metrics:\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Multilabel confusion matrices\n",
        "    conf_matrices = multilabel_confusion_matrix(all_targets, pred_labels)\n",
        "    for i, label in enumerate(label_names):\n",
        "        print(f\"\\nConfusion Matrix for '{label}':\")\n",
        "        print(conf_matrices[i])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3QkD-eWE1km0",
        "outputId": "9da18b07-1e65-40c4-b1ab-3593e699b4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Please upload your 'project_spectrogram_data.zip' file now:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-48f66257-c048-4d28-8903-643c44129d67\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-48f66257-c048-4d28-8903-643c44129d67\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving project_spectrogram_data.zip to project_spectrogram_data.zip\n",
            "✅ Extraction complete. Spectrogram dir: ./project_data/project_spectrogram_data/spectrogram\n",
            "✅ Data cleansing complete with 980 samples.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Training classifier head ---\n",
            "Epoch 1/5 Loss: 0.3085\n",
            "Epoch 2/5 Loss: 0.2238\n",
            "Epoch 3/5 Loss: 0.1931\n",
            "Epoch 4/5 Loss: 0.1528\n",
            "Epoch 5/5 Loss: 0.1112\n",
            "\n",
            "--- Phase 2: Fine-tuning entire model ---\n",
            "Epoch 1/50 Loss: 0.0773\n",
            "Epoch 2/50 Loss: 0.0587\n",
            "Epoch 3/50 Loss: 0.0431\n",
            "Epoch 4/50 Loss: 0.0345\n",
            "Epoch 5/50 Loss: 0.0255\n",
            "Epoch 6/50 Loss: 0.0224\n",
            "Epoch 7/50 Loss: 0.0153\n",
            "Epoch 8/50 Loss: 0.0156\n",
            "Epoch 9/50 Loss: 0.0129\n",
            "Epoch 10/50 Loss: 0.0159\n",
            "Epoch 11/50 Loss: 0.0119\n",
            "Epoch 12/50 Loss: 0.0073\n",
            "Epoch 13/50 Loss: 0.0101\n",
            "Epoch 14/50 Loss: 0.0083\n",
            "Epoch 15/50 Loss: 0.0059\n",
            "Epoch 16/50 Loss: 0.0050\n",
            "Epoch 17/50 Loss: 0.0035\n",
            "Epoch 18/50 Loss: 0.0046\n",
            "Epoch 19/50 Loss: 0.0034\n",
            "Epoch 20/50 Loss: 0.0036\n",
            "Epoch 21/50 Loss: 0.0020\n",
            "Epoch 22/50 Loss: 0.0035\n",
            "Epoch 23/50 Loss: 0.0030\n",
            "Epoch 24/50 Loss: 0.0031\n",
            "Epoch 25/50 Loss: 0.0020\n",
            "Epoch 26/50 Loss: 0.0036\n",
            "Epoch 27/50 Loss: 0.0024\n",
            "Epoch 28/50 Loss: 0.0018\n",
            "Epoch 29/50 Loss: 0.0019\n",
            "Epoch 30/50 Loss: 0.0007\n",
            "Epoch 31/50 Loss: 0.0019\n",
            "Epoch 32/50 Loss: 0.0011\n",
            "Epoch 33/50 Loss: 0.0011\n",
            "Epoch 34/50 Loss: 0.0012\n",
            "Epoch 35/50 Loss: 0.0004\n",
            "Epoch 36/50 Loss: 0.0007\n",
            "Epoch 37/50 Loss: 0.0009\n",
            "Epoch 38/50 Loss: 0.0007\n",
            "Epoch 39/50 Loss: 0.0010\n",
            "Epoch 40/50 Loss: 0.0008\n",
            "Epoch 41/50 Loss: 0.0003\n",
            "Epoch 42/50 Loss: 0.0004\n",
            "Epoch 43/50 Loss: 0.0004\n",
            "Epoch 44/50 Loss: 0.0004\n",
            "Epoch 45/50 Loss: 0.0002\n",
            "Epoch 46/50 Loss: 0.0006\n",
            "Epoch 47/50 Loss: 0.0002\n",
            "Epoch 48/50 Loss: 0.0004\n",
            "Epoch 49/50 Loss: 0.0003\n",
            "Epoch 50/50 Loss: 0.0008\n",
            "\n",
            "Overall Exact-Match Accuracy: 0.1633\n",
            "\n",
            "Overall Weighted Metrics:\n",
            "F1 Score: 0.5095\n",
            "Precision: 0.5745\n",
            "Recall: 0.4827\n",
            "\n",
            "Per-label Metrics:\n",
            "                                   Label  Accuracy  Precision    Recall  \\\n",
            "0                                Hip hop  0.867347   0.750000  0.416667   \n",
            "1                                    Rap  0.867347   0.681818  0.441176   \n",
            "2                             R&B / Soul  0.790816   0.560976  0.500000   \n",
            "3                                    Pop  0.698980   0.713178  0.807018   \n",
            "4                                   Rock  0.790816   0.578947  0.660000   \n",
            "5                                   Jazz  0.974490   0.000000  0.000000   \n",
            "6                                  Blues  0.994898   0.000000  0.000000   \n",
            "7                                Country  0.903061   0.761905  0.533333   \n",
            "8                                   Folk  0.933673   0.500000  0.153846   \n",
            "9                              Classical  0.989796   1.000000  0.333333   \n",
            "10                            Electronic  0.806122   0.500000  0.289474   \n",
            "11                                  Funk  1.000000   0.000000  0.000000   \n",
            "12                    Reggae / Ska / Dub  1.000000   0.000000  0.000000   \n",
            "13                                 Metal  0.954082   0.333333  0.125000   \n",
            "14                    Gospel / Religious  1.000000   0.000000  0.000000   \n",
            "15                                 Latin  0.989796   0.000000  0.000000   \n",
            "16                         Disco / Dance  0.857143   0.312500  0.227273   \n",
            "17             Ambient / Chill / New Age  0.928571   0.000000  0.000000   \n",
            "18            Experimental / Avant-Garde  0.964286   0.000000  0.000000   \n",
            "19  Opera / Musical Theater / Soundtrack  0.989796   0.000000  0.000000   \n",
            "20                    Vocal / A cappella  0.882653   0.571429  0.320000   \n",
            "21          Dancehall / Hip House / Club  0.994898   0.000000  0.000000   \n",
            "22                           Psychedelic  0.933673   0.333333  0.083333   \n",
            "23                         Other / Niche  0.974490   0.000000  0.000000   \n",
            "\n",
            "          F1  \n",
            "0   0.535714  \n",
            "1   0.535714  \n",
            "2   0.528736  \n",
            "3   0.757202  \n",
            "4   0.616822  \n",
            "5   0.000000  \n",
            "6   0.000000  \n",
            "7   0.627451  \n",
            "8   0.235294  \n",
            "9   0.500000  \n",
            "10  0.366667  \n",
            "11  0.000000  \n",
            "12  0.000000  \n",
            "13  0.181818  \n",
            "14  0.000000  \n",
            "15  0.000000  \n",
            "16  0.263158  \n",
            "17  0.000000  \n",
            "18  0.000000  \n",
            "19  0.000000  \n",
            "20  0.410256  \n",
            "21  0.000000  \n",
            "22  0.133333  \n",
            "23  0.000000  \n",
            "\n",
            "Confusion Matrix for 'Hip hop':\n",
            "[[155   5]\n",
            " [ 21  15]]\n",
            "\n",
            "Confusion Matrix for 'Rap':\n",
            "[[155   7]\n",
            " [ 19  15]]\n",
            "\n",
            "Confusion Matrix for 'R&B / Soul':\n",
            "[[132  18]\n",
            " [ 23  23]]\n",
            "\n",
            "Confusion Matrix for 'Pop':\n",
            "[[45 37]\n",
            " [22 92]]\n",
            "\n",
            "Confusion Matrix for 'Rock':\n",
            "[[122  24]\n",
            " [ 17  33]]\n",
            "\n",
            "Confusion Matrix for 'Jazz':\n",
            "[[191   1]\n",
            " [  4   0]]\n",
            "\n",
            "Confusion Matrix for 'Blues':\n",
            "[[195   0]\n",
            " [  1   0]]\n",
            "\n",
            "Confusion Matrix for 'Country':\n",
            "[[161   5]\n",
            " [ 14  16]]\n",
            "\n",
            "Confusion Matrix for 'Folk':\n",
            "[[181   2]\n",
            " [ 11   2]]\n",
            "\n",
            "Confusion Matrix for 'Classical':\n",
            "[[193   0]\n",
            " [  2   1]]\n",
            "\n",
            "Confusion Matrix for 'Electronic':\n",
            "[[147  11]\n",
            " [ 27  11]]\n",
            "\n",
            "Confusion Matrix for 'Funk':\n",
            "[[196   0]\n",
            " [  0   0]]\n",
            "\n",
            "Confusion Matrix for 'Reggae / Ska / Dub':\n",
            "[[196   0]\n",
            " [  0   0]]\n",
            "\n",
            "Confusion Matrix for 'Metal':\n",
            "[[186   2]\n",
            " [  7   1]]\n",
            "\n",
            "Confusion Matrix for 'Gospel / Religious':\n",
            "[[196   0]\n",
            " [  0   0]]\n",
            "\n",
            "Confusion Matrix for 'Latin':\n",
            "[[194   0]\n",
            " [  2   0]]\n",
            "\n",
            "Confusion Matrix for 'Disco / Dance':\n",
            "[[163  11]\n",
            " [ 17   5]]\n",
            "\n",
            "Confusion Matrix for 'Ambient / Chill / New Age':\n",
            "[[182   3]\n",
            " [ 11   0]]\n",
            "\n",
            "Confusion Matrix for 'Experimental / Avant-Garde':\n",
            "[[189   1]\n",
            " [  6   0]]\n",
            "\n",
            "Confusion Matrix for 'Opera / Musical Theater / Soundtrack':\n",
            "[[194   0]\n",
            " [  2   0]]\n",
            "\n",
            "Confusion Matrix for 'Vocal / A cappella':\n",
            "[[165   6]\n",
            " [ 17   8]]\n",
            "\n",
            "Confusion Matrix for 'Dancehall / Hip House / Club':\n",
            "[[195   0]\n",
            " [  1   0]]\n",
            "\n",
            "Confusion Matrix for 'Psychedelic':\n",
            "[[182   2]\n",
            " [ 11   1]]\n",
            "\n",
            "Confusion Matrix for 'Other / Niche':\n",
            "[[191   1]\n",
            " [  4   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EFFICIENT B0"
      ],
      "metadata": {
        "id": "07w2FTBdQa8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# --- CONFIG ---\n",
        "LOCAL_ZIP_NAME = 'project_spectrogram_data.zip'\n",
        "LOCAL_BASE_DIR = './project_data/'\n",
        "SPECTROGRAM_FOLDER_NAME = 'spectrogram'\n",
        "LOCAL_CSV_NAME = 'consolidated_genres.csv'\n",
        "CLEANED_CSV_NAME = 'cleaned_final_metadata.csv'\n",
        "SPECTROGRAM_FILE_EXTENSION = '.jpg'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_HEAD = 5\n",
        "EPOCHS_FULL = 75\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "SPECTROGRAM_DIR = None\n",
        "METADATA_FILE = None\n",
        "\n",
        "def upload_and_extract_data():\n",
        "    global SPECTROGRAM_DIR, METADATA_FILE\n",
        "    print(f\"Upload your '{LOCAL_ZIP_NAME}':\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"❌ No upload detected\")\n",
        "        return False\n",
        "    zip_filename = list(uploaded.keys())[0]\n",
        "    os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_BASE_DIR)\n",
        "    os.remove(zip_filename)\n",
        "    # Find paths\n",
        "    spectro_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', SPECTROGRAM_FOLDER_NAME), recursive=True)\n",
        "    csv_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)\n",
        "    if not spectro_paths or not csv_paths:\n",
        "        print(\"❌ Required data not found!\")\n",
        "        return False\n",
        "    SPECTROGRAM_DIR = spectro_paths[0]\n",
        "    METADATA_FILE = os.path.join(os.path.dirname(csv_paths[0]), CLEANED_CSV_NAME)\n",
        "    print(\"✅ Extraction successful.\")\n",
        "    return True\n",
        "\n",
        "def cleanse_metadata_file():\n",
        "    original_csv_path = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)[0]\n",
        "    df = pd.read_csv(original_csv_path)\n",
        "    # Fix extensions\n",
        "    df['filename'] = df['filename'].str.replace(r'\\.(png|jpeg|jpg)$', SPECTROGRAM_FILE_EXTENSION, regex=True)\n",
        "    files_set = set(os.listdir(SPECTROGRAM_DIR))\n",
        "    df = df[df['filename'].isin(files_set)].copy()\n",
        "    label_cols = df.columns[1:]\n",
        "    # Remove if no labels\n",
        "    df['label_sum'] = df[label_cols].sum(axis=1)\n",
        "    df = df[df['label_sum'] > 0].drop(columns=['label_sum'])\n",
        "    df.to_csv(METADATA_FILE, index=False)\n",
        "    print(f\"✅ Data cleansed: {len(df)} samples, {len(label_cols)} labels\")\n",
        "    return label_cols\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self.df.columns[1:].tolist()\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.df.iloc[idx,0])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        label = self.df.iloc[idx,1:].values.astype(np.float32)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label)\n",
        "\n",
        "# Data Augmentations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.5, 0.5, 0.5, 0.15),\n",
        "    transforms.RandomErasing(p=0.7),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3,1,1) if x.size(0) == 1 else x),\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3,1,1) if x.size(0) == 1 else x),\n",
        "])\n",
        "\n",
        "def train():\n",
        "    if not upload_and_extract_data():\n",
        "        return\n",
        "    labels = cleanse_metadata_file()\n",
        "    dataset = SpectrogramDataset(METADATA_FILE, SPECTROGRAM_DIR, transform=None)\n",
        "    train_len = int(len(dataset)*0.8)\n",
        "    val_len = len(dataset) - train_len\n",
        "    train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
        "    train_ds.dataset.transform = train_transform\n",
        "    val_ds.dataset.transform = val_transform\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Load model\n",
        "    model = models.efficientnet_b0(pretrained=True)\n",
        "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(labels))\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3,\n",
        "                                              steps_per_epoch=len(train_loader), epochs=EPOCHS_HEAD+EPOCHS_FULL)\n",
        "\n",
        "    # Phase 1: Train only classifier head\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"Training classifier head...\")\n",
        "    for epoch in range(EPOCHS_HEAD):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for imgs, targets in train_loader:\n",
        "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "        print(f\"Head Epoch {epoch+1} Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Phase 2: Fine-tune entire model\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = True\n",
        "    print(\"Fine-tuning entire model...\")\n",
        "    for epoch in range(EPOCHS_FULL):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for imgs, targets in train_loader:\n",
        "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "        print(f\"Fine-tune Epoch {epoch+1} Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in val_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            outputs = model(imgs)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_targets.append(targets.numpy())\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    pred_labels = (all_preds > 0.5).astype(int)\n",
        "\n",
        "    from sklearn.metrics import classification_report, f1_score\n",
        "    weighted_f1 = f1_score(all_targets, pred_labels, average='weighted', zero_division=0)\n",
        "    print(f\"\\nWeighted F1 score: {weighted_f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(classification_report(all_targets, pred_labels, target_names=labels, zero_division=0))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gXuc0RnW8Vnu",
        "outputId": "efbc79e6-9932-4378-ae04-e4c9ca0af65a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Upload your 'project_spectrogram_data.zip':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e59c1677-c8a2-4612-8df1-cf77a525e0df\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e59c1677-c8a2-4612-8df1-cf77a525e0df\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving project_spectrogram_data.zip to project_spectrogram_data.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction successful.\n",
            "✅ Data cleansed: 980 samples, 24 labels\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 114MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier head...\n",
            "Head Epoch 1 Loss: 0.6583\n",
            "Head Epoch 2 Loss: 0.5846\n",
            "Head Epoch 3 Loss: 0.5128\n",
            "Head Epoch 4 Loss: 0.4410\n",
            "Head Epoch 5 Loss: 0.3815\n",
            "Fine-tuning entire model...\n",
            "Fine-tune Epoch 1 Loss: 0.2761\n",
            "Fine-tune Epoch 2 Loss: 0.2122\n",
            "Fine-tune Epoch 3 Loss: 0.1780\n",
            "Fine-tune Epoch 4 Loss: 0.1509\n",
            "Fine-tune Epoch 5 Loss: 0.1278\n",
            "Fine-tune Epoch 6 Loss: 0.1081\n",
            "Fine-tune Epoch 7 Loss: 0.0953\n",
            "Fine-tune Epoch 8 Loss: 0.0821\n",
            "Fine-tune Epoch 9 Loss: 0.0757\n",
            "Fine-tune Epoch 10 Loss: 0.0691\n",
            "Fine-tune Epoch 11 Loss: 0.0602\n",
            "Fine-tune Epoch 12 Loss: 0.0613\n",
            "Fine-tune Epoch 13 Loss: 0.0595\n",
            "Fine-tune Epoch 14 Loss: 0.0480\n",
            "Fine-tune Epoch 15 Loss: 0.0431\n",
            "Fine-tune Epoch 16 Loss: 0.0337\n",
            "Fine-tune Epoch 17 Loss: 0.0249\n",
            "Fine-tune Epoch 18 Loss: 0.0255\n",
            "Fine-tune Epoch 19 Loss: 0.0241\n",
            "Fine-tune Epoch 20 Loss: 0.0202\n",
            "Fine-tune Epoch 21 Loss: 0.0177\n",
            "Fine-tune Epoch 22 Loss: 0.0142\n",
            "Fine-tune Epoch 23 Loss: 0.0149\n",
            "Fine-tune Epoch 24 Loss: 0.0142\n",
            "Fine-tune Epoch 25 Loss: 0.0145\n",
            "Fine-tune Epoch 26 Loss: 0.0141\n",
            "Fine-tune Epoch 27 Loss: 0.0182\n",
            "Fine-tune Epoch 28 Loss: 0.0195\n",
            "Fine-tune Epoch 29 Loss: 0.0190\n",
            "Fine-tune Epoch 30 Loss: 0.0195\n",
            "Fine-tune Epoch 31 Loss: 0.0118\n",
            "Fine-tune Epoch 32 Loss: 0.0107\n",
            "Fine-tune Epoch 33 Loss: 0.0084\n",
            "Fine-tune Epoch 34 Loss: 0.0075\n",
            "Fine-tune Epoch 35 Loss: 0.0075\n",
            "Fine-tune Epoch 36 Loss: 0.0050\n",
            "Fine-tune Epoch 37 Loss: 0.0045\n",
            "Fine-tune Epoch 38 Loss: 0.0058\n",
            "Fine-tune Epoch 39 Loss: 0.0050\n",
            "Fine-tune Epoch 40 Loss: 0.0037\n",
            "Fine-tune Epoch 41 Loss: 0.0031\n",
            "Fine-tune Epoch 42 Loss: 0.0029\n",
            "Fine-tune Epoch 43 Loss: 0.0028\n",
            "Fine-tune Epoch 44 Loss: 0.0020\n",
            "Fine-tune Epoch 45 Loss: 0.0025\n",
            "Fine-tune Epoch 46 Loss: 0.0020\n",
            "Fine-tune Epoch 47 Loss: 0.0020\n",
            "Fine-tune Epoch 48 Loss: 0.0019\n",
            "Fine-tune Epoch 49 Loss: 0.0016\n",
            "Fine-tune Epoch 50 Loss: 0.0017\n",
            "Fine-tune Epoch 51 Loss: 0.0012\n",
            "Fine-tune Epoch 52 Loss: 0.0012\n",
            "Fine-tune Epoch 53 Loss: 0.0012\n",
            "Fine-tune Epoch 54 Loss: 0.0011\n",
            "Fine-tune Epoch 55 Loss: 0.0014\n",
            "Fine-tune Epoch 56 Loss: 0.0010\n",
            "Fine-tune Epoch 57 Loss: 0.0015\n",
            "Fine-tune Epoch 58 Loss: 0.0011\n",
            "Fine-tune Epoch 59 Loss: 0.0013\n",
            "Fine-tune Epoch 60 Loss: 0.0012\n",
            "Fine-tune Epoch 61 Loss: 0.0012\n",
            "Fine-tune Epoch 62 Loss: 0.0007\n",
            "Fine-tune Epoch 63 Loss: 0.0009\n",
            "Fine-tune Epoch 64 Loss: 0.0009\n",
            "Fine-tune Epoch 65 Loss: 0.0009\n",
            "Fine-tune Epoch 66 Loss: 0.0009\n",
            "Fine-tune Epoch 67 Loss: 0.0010\n",
            "Fine-tune Epoch 68 Loss: 0.0008\n",
            "Fine-tune Epoch 69 Loss: 0.0009\n",
            "Fine-tune Epoch 70 Loss: 0.0009\n",
            "Fine-tune Epoch 71 Loss: 0.0009\n",
            "Fine-tune Epoch 72 Loss: 0.0010\n",
            "Fine-tune Epoch 73 Loss: 0.0011\n",
            "Fine-tune Epoch 74 Loss: 0.0010\n",
            "Fine-tune Epoch 75 Loss: 0.0009\n",
            "\n",
            "Weighted F1 score: 0.5863\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                             Hip hop       0.78      0.70      0.74        44\n",
            "                                 Rap       0.74      0.65      0.69        43\n",
            "                          R&B / Soul       0.71      0.64      0.67        47\n",
            "                                 Pop       0.77      0.81      0.79       108\n",
            "                                Rock       0.68      0.75      0.71        48\n",
            "                                Jazz       0.00      0.00      0.00         7\n",
            "                               Blues       0.00      0.00      0.00         1\n",
            "                             Country       0.70      0.76      0.73        25\n",
            "                                Folk       0.50      0.55      0.52        11\n",
            "                           Classical       0.67      1.00      0.80         2\n",
            "                          Electronic       0.59      0.24      0.34        41\n",
            "                                Funk       0.00      0.00      0.00         1\n",
            "                  Reggae / Ska / Dub       0.00      0.00      0.00         1\n",
            "                               Metal       1.00      0.40      0.57         5\n",
            "                  Gospel / Religious       0.00      0.00      0.00         1\n",
            "                               Latin       0.00      0.00      0.00         2\n",
            "                       Disco / Dance       0.64      0.30      0.41        23\n",
            "           Ambient / Chill / New Age       0.40      0.15      0.22        13\n",
            "          Experimental / Avant-Garde       0.00      0.00      0.00        15\n",
            "Opera / Musical Theater / Soundtrack       0.00      0.00      0.00         1\n",
            "                  Vocal / A cappella       0.53      0.40      0.46        20\n",
            "        Dancehall / Hip House / Club       0.00      0.00      0.00         2\n",
            "                         Psychedelic       1.00      0.09      0.17        11\n",
            "                       Other / Niche       0.00      0.00      0.00         4\n",
            "\n",
            "                           micro avg       0.71      0.57      0.63       476\n",
            "                           macro avg       0.40      0.31      0.33       476\n",
            "                        weighted avg       0.65      0.57      0.59       476\n",
            "                         samples avg       0.71      0.62      0.63       476\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EFFICIENTNET B3"
      ],
      "metadata": {
        "id": "kaA3pZqGQMHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "# -------------- CONFIG --------------\n",
        "LOCAL_ZIP_NAME = 'project_spectrogram_data.zip'\n",
        "LOCAL_BASE_DIR = './project_data/'\n",
        "SPECTROGRAM_FOLDER_NAME = 'spectrogram'\n",
        "LOCAL_CSV_NAME = 'consolidated_genres.csv'\n",
        "CLEANED_CSV_NAME = 'cleaned_final_metadata.csv'\n",
        "SPECTROGRAM_FILE_EXTENSION = '.jpg'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_HEAD = 8\n",
        "EPOCHS_FULL = 90\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# -------------- DATA PREP --------------\n",
        "SPECTROGRAM_DIR = None\n",
        "METADATA_FILE = None\n",
        "\n",
        "def upload_and_extract():\n",
        "    global SPECTROGRAM_DIR, METADATA_FILE\n",
        "    print(f\"Upload '{LOCAL_ZIP_NAME}' now:\")\n",
        "    uploaded = files.upload()\n",
        "    zip_filename = list(uploaded.keys())[0]\n",
        "    os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_BASE_DIR)\n",
        "    os.remove(zip_filename)\n",
        "    spectros = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', SPECTROGRAM_FOLDER_NAME), recursive=True)\n",
        "    csvs = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)\n",
        "    if not spectros or not csvs:\n",
        "        raise RuntimeError(\"Spectrogram folder or CSV not found after extraction!\")\n",
        "    SPECTROGRAM_DIR = spectros[0]\n",
        "    METADATA_FILE = os.path.join(os.path.dirname(csvs[0]), CLEANED_CSV_NAME)\n",
        "\n",
        "def cleanse_metadata():\n",
        "    orig_csv = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)[0]\n",
        "    df = pd.read_csv(orig_csv)\n",
        "    df['filename'] = df['filename'].str.replace(r'\\.(png|jpeg|jpg)$', SPECTROGRAM_FILE_EXTENSION, regex=True)\n",
        "    valid_files = set(os.listdir(SPECTROGRAM_DIR))\n",
        "    df = df[df['filename'].isin(valid_files)].copy()\n",
        "    label_cols = df.columns[1:]\n",
        "    df['label_sum'] = df[label_cols].sum(axis=1)\n",
        "    df = df[df['label_sum'] > 0].drop(columns=['label_sum'])\n",
        "    df.to_csv(METADATA_FILE, index=False)\n",
        "    print(f\"✅ {len(df)} samples after cleaning. {len(label_cols)} labels.\")\n",
        "    return label_cols\n",
        "\n",
        "class SpectrogramDS(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self.df.columns[1:].tolist()\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        img_fp = os.path.join(self.img_dir, self.df.iloc[idx,0])\n",
        "        img = Image.open(img_fp).convert('RGB')\n",
        "        label = self.df.iloc[idx,1:].values.astype(np.float32)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# -------------- AUGMENTATION --------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(300, scale=(0.35, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.ColorJitter(.6, .6, .6, .2),\n",
        "    transforms.RandomErasing(p=0.8, value='random'),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3,1,1) if x.size(0)==1 else x),\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((300, 300)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3,1,1) if x.size(0)==1 else x)\n",
        "])\n",
        "\n",
        "# -------------- ADVANCED MODEL (EfficientNet-B3) --------------\n",
        "# If not installed: pip install timm\n",
        "import timm\n",
        "def get_model(num_classes):\n",
        "    model = timm.create_model('efficientnet_b3a', pretrained=True)\n",
        "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "# -------------- TRAINING + EVAL --------------\n",
        "def get_class_weights(ds):\n",
        "    y = pd.read_csv(METADATA_FILE).iloc[:,1:].values\n",
        "    pos_weights = 1. / (np.sum(y, axis=0) + 1e-4)\n",
        "    pos_weights = torch.tensor(pos_weights, dtype=torch.float32).to(DEVICE)\n",
        "    return pos_weights\n",
        "\n",
        "def tune_thresholds(y_true, y_pred):\n",
        "    # Tune per-class threshold to maximize f1\n",
        "    from sklearn.metrics import f1_score\n",
        "    thresholds = np.arange(0.3, 0.7, 0.02)\n",
        "    best_thr = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        best = 0.5; bestf1=0\n",
        "        for t in thresholds:\n",
        "            f = f1_score(y_true[:,i], (y_pred[:,i]>=t).astype(int), zero_division=0)\n",
        "            if f > bestf1:\n",
        "                bestf1 = f\n",
        "                best = t\n",
        "        best_thr.append(best)\n",
        "    return np.array(best_thr)\n",
        "\n",
        "def main():\n",
        "    upload_and_extract()\n",
        "    label_cols = cleanse_metadata()\n",
        "    dataset = SpectrogramDS(METADATA_FILE, SPECTROGRAM_DIR, transform=None)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "    train_ds.dataset.transform = train_transform\n",
        "    val_ds.dataset.transform = val_transform\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Model & optimizer\n",
        "    model = get_model(len(label_cols))\n",
        "    weights = get_class_weights(dataset)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-3,\n",
        "                                              steps_per_epoch=len(train_loader),\n",
        "                                              epochs=EPOCHS_HEAD+EPOCHS_FULL)\n",
        "\n",
        "    # Freeze backbone for head train\n",
        "    for p in model.parameters(): p.requires_grad = False\n",
        "    for p in model.classifier.parameters(): p.requires_grad = True\n",
        "\n",
        "    print(\"Phase 1: Classifier head...\")\n",
        "    for epoch in range(EPOCHS_HEAD):\n",
        "        model.train(); epoch_loss=0\n",
        "        for imgs, ys in train_loader:\n",
        "            imgs, ys = imgs.to(DEVICE), ys.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outs = model(imgs)\n",
        "            loss = criterion(outs, ys)\n",
        "            loss.backward(); optimizer.step(); scheduler.step()\n",
        "            epoch_loss += loss.item() * imgs.size(0)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS_HEAD}, Loss: {epoch_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Unfreeze whole model\n",
        "    for p in model.parameters(): p.requires_grad = True\n",
        "\n",
        "    print(\"Phase 2: Full fine-tuning...\")\n",
        "    for epoch in range(EPOCHS_FULL):\n",
        "        model.train(); epoch_loss=0\n",
        "        for imgs, ys in train_loader:\n",
        "            imgs, ys = imgs.to(DEVICE), ys.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outs = model(imgs)\n",
        "            loss = criterion(outs, ys)\n",
        "            loss.backward(); optimizer.step(); scheduler.step()\n",
        "            epoch_loss += loss.item() * imgs.size(0)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS_FULL}, Loss: {epoch_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, ys in val_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            outs = torch.sigmoid(model(imgs)).cpu().numpy()\n",
        "            all_preds.append(outs)\n",
        "            all_targets.append(ys.numpy())\n",
        "    all_preds, all_targets = np.vstack(all_preds), np.vstack(all_targets)\n",
        "\n",
        "    # ----------- Threshold tuning -----------\n",
        "    best_thr = tune_thresholds(all_targets, all_preds)\n",
        "    tuned_pred = (all_preds >= best_thr).astype(int)\n",
        "    f1 = f1_score(all_targets, tuned_pred, average='weighted', zero_division=0)\n",
        "    print(f\"\\nWeighted F1 after threshold tuning: {f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(classification_report(all_targets, tuned_pred, target_names=label_cols, zero_division=0))\n",
        "    exact_match = np.mean(np.all(tuned_pred == all_targets, axis=1))\n",
        "    print(f\"\\nExact Match Accuracy: {exact_match:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "N2k9h1GpJy8o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4c8d4e04ac5c4e8e89be2b1f4036cf85",
            "ccb44c8a70ce43f9937e0ddfcf09c64c",
            "4366176900774ae5976e0e5c1e1aed52",
            "9cfd165f07b1404fb63919748b941012",
            "19c6bb40d9244265a545345d6a52f8e3",
            "4f3478addb854ca1a8c04f9581c0adb7",
            "d355ee76980a49f2b1d8a032947ff2a2",
            "37caa8a39d0b4a43afc5f0b66ca26fbb",
            "3984f65d5e9e4c9eb2f3cc1c45eb38ae",
            "229173e87720455d95fb5b7e8728a204",
            "2bd50c2cff2442619e06e119f1ca547a"
          ]
        },
        "outputId": "5f98dfd1-b001-4088-e15b-901b146eb1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload 'project_spectrogram_data.zip' now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-340a6af7-f348-4bb0-b0d5-83c6c24bc798\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-340a6af7-f348-4bb0-b0d5-83c6c24bc798\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving project_spectrogram_data.zip to project_spectrogram_data.zip\n",
            "✅ 980 samples after cleaning. 24 labels.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name efficientnet_b3a to current efficientnet_b3.\n",
            "  model = create_fn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c8d4e04ac5c4e8e89be2b1f4036cf85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 1: Classifier head...\n",
            "Epoch 1/8, Loss: 0.5486\n",
            "Epoch 2/8, Loss: 0.3943\n",
            "Epoch 3/8, Loss: 0.2704\n",
            "Epoch 4/8, Loss: 0.1816\n",
            "Epoch 5/8, Loss: 0.1188\n",
            "Epoch 6/8, Loss: 0.0806\n",
            "Epoch 7/8, Loss: 0.0576\n",
            "Epoch 8/8, Loss: 0.0433\n",
            "Phase 2: Full fine-tuning...\n",
            "Epoch 1/90, Loss: 0.0110\n",
            "Epoch 2/90, Loss: 0.0077\n",
            "Epoch 3/90, Loss: 0.0075\n",
            "Epoch 4/90, Loss: 0.0078\n",
            "Epoch 5/90, Loss: 0.0076\n",
            "Epoch 6/90, Loss: 0.0075\n",
            "Epoch 7/90, Loss: 0.0079\n",
            "Epoch 8/90, Loss: 0.0075\n",
            "Epoch 9/90, Loss: 0.0070\n",
            "Epoch 10/90, Loss: 0.0069\n",
            "Epoch 11/90, Loss: 0.0069\n",
            "Epoch 12/90, Loss: 0.0063\n",
            "Epoch 13/90, Loss: 0.0067\n",
            "Epoch 14/90, Loss: 0.0066\n",
            "Epoch 15/90, Loss: 0.0062\n",
            "Epoch 16/90, Loss: 0.0050\n",
            "Epoch 17/90, Loss: 0.0045\n",
            "Epoch 18/90, Loss: 0.0040\n",
            "Epoch 19/90, Loss: 0.0044\n",
            "Epoch 20/90, Loss: 0.0050\n",
            "Epoch 21/90, Loss: 0.0050\n",
            "Epoch 22/90, Loss: 0.0043\n",
            "Epoch 23/90, Loss: 0.0044\n",
            "Epoch 24/90, Loss: 0.0049\n",
            "Epoch 25/90, Loss: 0.0058\n",
            "Epoch 26/90, Loss: 0.0057\n",
            "Epoch 27/90, Loss: 0.0050\n",
            "Epoch 28/90, Loss: 0.0042\n",
            "Epoch 29/90, Loss: 0.0040\n",
            "Epoch 30/90, Loss: 0.0033\n",
            "Epoch 31/90, Loss: 0.0034\n",
            "Epoch 32/90, Loss: 0.0030\n",
            "Epoch 33/90, Loss: 0.0033\n",
            "Epoch 34/90, Loss: 0.0031\n",
            "Epoch 35/90, Loss: 0.0029\n",
            "Epoch 36/90, Loss: 0.0026\n",
            "Epoch 37/90, Loss: 0.0029\n",
            "Epoch 38/90, Loss: 0.0033\n",
            "Epoch 39/90, Loss: 0.0030\n",
            "Epoch 40/90, Loss: 0.0035\n",
            "Epoch 41/90, Loss: 0.0037\n",
            "Epoch 42/90, Loss: 0.0031\n",
            "Epoch 43/90, Loss: 0.0023\n",
            "Epoch 44/90, Loss: 0.0020\n",
            "Epoch 45/90, Loss: 0.0017\n",
            "Epoch 46/90, Loss: 0.0015\n",
            "Epoch 47/90, Loss: 0.0013\n",
            "Epoch 48/90, Loss: 0.0013\n",
            "Epoch 49/90, Loss: 0.0014\n",
            "Epoch 50/90, Loss: 0.0023\n",
            "Epoch 51/90, Loss: 0.0027\n",
            "Epoch 52/90, Loss: 0.0027\n",
            "Epoch 53/90, Loss: 0.0029\n",
            "Epoch 54/90, Loss: 0.0020\n",
            "Epoch 55/90, Loss: 0.0017\n",
            "Epoch 56/90, Loss: 0.0015\n",
            "Epoch 57/90, Loss: 0.0013\n",
            "Epoch 58/90, Loss: 0.0011\n",
            "Epoch 59/90, Loss: 0.0010\n",
            "Epoch 60/90, Loss: 0.0010\n",
            "Epoch 61/90, Loss: 0.0008\n",
            "Epoch 62/90, Loss: 0.0008\n",
            "Epoch 63/90, Loss: 0.0008\n",
            "Epoch 64/90, Loss: 0.0009\n",
            "Epoch 65/90, Loss: 0.0008\n",
            "Epoch 66/90, Loss: 0.0008\n",
            "Epoch 67/90, Loss: 0.0007\n",
            "Epoch 68/90, Loss: 0.0007\n",
            "Epoch 69/90, Loss: 0.0007\n",
            "Epoch 70/90, Loss: 0.0006\n",
            "Epoch 71/90, Loss: 0.0006\n",
            "Epoch 72/90, Loss: 0.0006\n",
            "Epoch 73/90, Loss: 0.0006\n",
            "Epoch 74/90, Loss: 0.0006\n",
            "Epoch 75/90, Loss: 0.0005\n",
            "Epoch 76/90, Loss: 0.0005\n",
            "Epoch 77/90, Loss: 0.0005\n",
            "Epoch 78/90, Loss: 0.0006\n",
            "Epoch 79/90, Loss: 0.0005\n",
            "Epoch 80/90, Loss: 0.0005\n",
            "Epoch 81/90, Loss: 0.0005\n",
            "Epoch 82/90, Loss: 0.0005\n",
            "Epoch 83/90, Loss: 0.0005\n",
            "Epoch 84/90, Loss: 0.0005\n",
            "Epoch 85/90, Loss: 0.0005\n",
            "Epoch 86/90, Loss: 0.0005\n",
            "Epoch 87/90, Loss: 0.0005\n",
            "Epoch 88/90, Loss: 0.0005\n",
            "Epoch 89/90, Loss: 0.0005\n",
            "Epoch 90/90, Loss: 0.0005\n",
            "\n",
            "Weighted F1 after threshold tuning: 0.4262\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                             Hip hop       0.82      0.40      0.54        35\n",
            "                                 Rap       1.00      0.26      0.41        35\n",
            "                          R&B / Soul       0.58      0.47      0.52        38\n",
            "                                 Pop       0.87      0.54      0.67       114\n",
            "                                Rock       0.74      0.39      0.51        51\n",
            "                                Jazz       0.00      0.00      0.00         4\n",
            "                               Blues       0.00      0.00      0.00         1\n",
            "                             Country       0.86      0.49      0.62        37\n",
            "                                Folk       1.00      0.12      0.22        16\n",
            "                           Classical       1.00      0.75      0.86         4\n",
            "                          Electronic       0.00      0.00      0.00        31\n",
            "                                Funk       0.00      0.00      0.00         0\n",
            "                  Reggae / Ska / Dub       0.00      0.00      0.00         1\n",
            "                               Metal       1.00      0.33      0.50         3\n",
            "                  Gospel / Religious       0.00      0.00      0.00         1\n",
            "                               Latin       0.00      0.00      0.00         1\n",
            "                       Disco / Dance       0.00      0.00      0.00        17\n",
            "           Ambient / Chill / New Age       0.50      0.18      0.27        11\n",
            "          Experimental / Avant-Garde       0.00      0.00      0.00        11\n",
            "Opera / Musical Theater / Soundtrack       0.00      0.00      0.00         2\n",
            "                  Vocal / A cappella       0.50      0.04      0.08        23\n",
            "        Dancehall / Hip House / Club       0.00      0.00      0.00         2\n",
            "                         Psychedelic       0.00      0.00      0.00         9\n",
            "                       Other / Niche       0.00      0.00      0.00         3\n",
            "\n",
            "                           micro avg       0.80      0.33      0.47       450\n",
            "                           macro avg       0.37      0.17      0.22       450\n",
            "                        weighted avg       0.66      0.33      0.43       450\n",
            "                         samples avg       0.60      0.38      0.44       450\n",
            "\n",
            "\n",
            "Exact Match Accuracy: 0.1888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# CONFIG\n",
        "LOCAL_ZIP_NAME = 'project_spectrogram_data.zip'\n",
        "LOCAL_BASE_DIR = './project_data/'\n",
        "SPECTROGRAM_FOLDER_NAME = 'spectrogram'\n",
        "LOCAL_CSV_NAME = 'consolidated_genres.csv'\n",
        "CLEANED_CSV_NAME = 'cleaned_final_metadata.csv'\n",
        "SPECTROGRAM_FILE_EXTENSION = '.jpg'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_HEAD = 5\n",
        "EPOCHS_FULL = 100\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SPECTROGRAM_DIR = None\n",
        "METADATA_FILE = None\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=0.5):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return F_loss.mean()\n",
        "\n",
        "def upload_and_extract_data():\n",
        "    global SPECTROGRAM_DIR, METADATA_FILE\n",
        "    print(f\"Upload your '{LOCAL_ZIP_NAME}':\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"❌ No upload detected\")\n",
        "        return False\n",
        "    zip_filename = list(uploaded.keys())[0]\n",
        "    os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_BASE_DIR)\n",
        "    os.remove(zip_filename)\n",
        "    spectro_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', SPECTROGRAM_FOLDER_NAME), recursive=True)\n",
        "    csv_paths = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)\n",
        "    if not spectro_paths or not csv_paths:\n",
        "        print(\"❌ Required data not found!\")\n",
        "        return False\n",
        "    SPECTROGRAM_DIR = spectro_paths[0]\n",
        "    METADATA_FILE = os.path.join(os.path.dirname(csv_paths[0]), CLEANED_CSV_NAME)\n",
        "    print(\"✅ Extraction successful.\")\n",
        "    return True\n",
        "\n",
        "def cleanse_metadata_file():\n",
        "    original_csv_path = glob.glob(os.path.join(LOCAL_BASE_DIR, '**', LOCAL_CSV_NAME), recursive=True)[0]\n",
        "    df = pd.read_csv(original_csv_path)\n",
        "    df['filename'] = df['filename'].str.replace(r'\\.(png|jpeg|jpg)$', SPECTROGRAM_FILE_EXTENSION, regex=True)\n",
        "    files_set = set(os.listdir(SPECTROGRAM_DIR))\n",
        "    df = df[df['filename'].isin(files_set)].copy()\n",
        "    label_cols = df.columns[1:]\n",
        "    df['label_sum'] = df[label_cols].sum(axis=1)\n",
        "    df = df[df['label_sum'] > 0].drop(columns=['label_sum'])\n",
        "    df.to_csv(METADATA_FILE, index=False)\n",
        "    print(f\"✅ Data cleansed: {len(df)} samples, {len(label_cols)} labels\")\n",
        "    return label_cols\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self.df.columns[1:].tolist()\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.df.iloc[idx,0])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        label = self.df.iloc[idx,1:].values.astype(np.float32)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(300, scale=(0.4, 1.0), ratio=(0.75, 1.33)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.15),\n",
        "    transforms.GaussianBlur(kernel_size=7, sigma=(0.1, 2.5)),\n",
        "    transforms.RandomErasing(p=0.8, scale=(0.02, 0.2), ratio=(0.2, 2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3,1,1) if x.size(0) == 1 else x),\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((300, 300)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n",
        "    transforms.Lambda(lambda x: x.repeat(3,1,1) if x.size(0) == 1 else x),\n",
        "])\n",
        "\n",
        "def eval_metrics(val_targets, val_preds, label_names):\n",
        "    from sklearn.metrics import f1_score, classification_report\n",
        "    # Threshold sweep\n",
        "    best_thresholds = []\n",
        "    best_f1s = []\n",
        "    for i in range(val_targets.shape[1]):\n",
        "        best_f1, best_th = 0, 0.5\n",
        "        for th in np.arange(0.1, 0.91, 0.01):\n",
        "            f1 = f1_score(val_targets[:,i], (val_preds[:,i]>th).astype(int), zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_th = f1, th\n",
        "        best_thresholds.append(best_th)\n",
        "        best_f1s.append(best_f1)\n",
        "    print(\"Best F1/Threshold per class:\")\n",
        "    for n, f, t in zip(label_names, best_f1s, best_thresholds):\n",
        "        print(f\"{n:35} F1={f:.3f}  th={t:.2f}\")\n",
        "    # Apply per-class best thresholds\n",
        "    final_preds = np.zeros_like(val_preds)\n",
        "    for i, th in enumerate(best_thresholds):\n",
        "        final_preds[:,i] = (val_preds[:,i] > th).astype(int)\n",
        "    exact_match = np.all(final_preds == val_targets, axis=1).mean()\n",
        "    weighted_f1 = f1_score(val_targets, final_preds, average='weighted', zero_division=0)\n",
        "    micro_f1 = f1_score(val_targets, final_preds, average='micro', zero_division=0)\n",
        "    print(f\"\\nExact-match accuracy: {exact_match:.4f} ({exact_match*100:.2f}%)\")\n",
        "    print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
        "    print(f\"Micro F1: {micro_f1:.4f}\\n\")\n",
        "    print(\"Classification report:\\n\")\n",
        "    print(classification_report(val_targets, final_preds, target_names=label_names, zero_division=0))\n",
        "    return exact_match, weighted_f1, micro_f1\n",
        "\n",
        "def train():\n",
        "    if not upload_and_extract_data():\n",
        "        return\n",
        "    labels = cleanse_metadata_file()\n",
        "    dataset = SpectrogramDataset(METADATA_FILE, SPECTROGRAM_DIR, transform=None)\n",
        "    train_len = int(len(dataset)*0.8)\n",
        "    val_len = len(dataset) - train_len\n",
        "    train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
        "    train_ds.dataset.transform = train_transform\n",
        "    val_ds.dataset.transform = val_transform\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = models.efficientnet_b3(pretrained=True)\n",
        "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(labels))\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = FocalLoss(gamma=2, alpha=0.5)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=2e-4)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-4, steps_per_epoch=len(train_loader), epochs=EPOCHS_HEAD+EPOCHS_FULL)\n",
        "\n",
        "    # Head training\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"Training classifier head...\")\n",
        "    for epoch in range(EPOCHS_HEAD):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for imgs, targets in train_loader:\n",
        "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "        print(f\"Head Epoch {epoch+1} Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Phase 2: Fine-tune\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = True\n",
        "    best_f1 = 0\n",
        "    print(\"Fine-tuning entire model...\")\n",
        "    for epoch in range(EPOCHS_FULL):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for imgs, targets in train_loader:\n",
        "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "        print(f\"Fine-tune Epoch {epoch+1} Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "        # Validation epoch\n",
        "        model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, targets in val_loader:\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                outputs = model(imgs)\n",
        "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(targets.numpy())\n",
        "        all_preds = np.vstack(all_preds)\n",
        "        all_targets = np.vstack(all_targets)\n",
        "        pred_labels = (all_preds > 0.5).astype(int)\n",
        "        weighted_f1 = f1_score(all_targets, pred_labels, average='weighted', zero_division=0)\n",
        "        if weighted_f1 > best_f1:\n",
        "            best_f1 = weighted_f1\n",
        "            torch.save(model.state_dict(), 'best_focal_efficientnet_b3.pth')\n",
        "            print(f\"  (New best model saved, val Weighted F1: {weighted_f1:.4f})\")\n",
        "\n",
        "    # Final Evaluation with best thresholds\n",
        "    print(\"Evaluating best model with sweeping thresholds:\")\n",
        "    model.load_state_dict(torch.load('best_focal_efficientnet_b3.pth'))\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in val_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            outputs = model(imgs)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_targets.append(targets.numpy())\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    eval_metrics(all_targets, all_preds, labels)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "U62-EeIjvAEb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fba3538-42ed-47cd-f3df-840ca971975c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your 'project_spectrogram_data.zip':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1d0a532e-6182-4bd2-a70c-0e5c217c2b5f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1d0a532e-6182-4bd2-a70c-0e5c217c2b5f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving project_spectrogram_data.zip to project_spectrogram_data.zip\n",
            "✅ Extraction successful.\n",
            "✅ Data cleansed: 980 samples, 24 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47.2M/47.2M [00:00<00:00, 210MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier head...\n",
            "Head Epoch 1 Loss: 0.0856\n",
            "Head Epoch 2 Loss: 0.0758\n",
            "Head Epoch 3 Loss: 0.0671\n",
            "Head Epoch 4 Loss: 0.0588\n",
            "Head Epoch 5 Loss: 0.0517\n",
            "Fine-tuning entire model...\n",
            "Fine-tune Epoch 1 Loss: 0.0420\n",
            "  (New best model saved, val Weighted F1: 0.1731)\n",
            "Fine-tune Epoch 2 Loss: 0.0343\n",
            "  (New best model saved, val Weighted F1: 0.1862)\n",
            "Fine-tune Epoch 3 Loss: 0.0302\n",
            "  (New best model saved, val Weighted F1: 0.2825)\n",
            "Fine-tune Epoch 4 Loss: 0.0271\n",
            "  (New best model saved, val Weighted F1: 0.3761)\n",
            "Fine-tune Epoch 5 Loss: 0.0241\n",
            "  (New best model saved, val Weighted F1: 0.4147)\n",
            "Fine-tune Epoch 6 Loss: 0.0212\n",
            "  (New best model saved, val Weighted F1: 0.4811)\n",
            "Fine-tune Epoch 7 Loss: 0.0185\n",
            "  (New best model saved, val Weighted F1: 0.4851)\n",
            "Fine-tune Epoch 8 Loss: 0.0164\n",
            "Fine-tune Epoch 9 Loss: 0.0136\n",
            "  (New best model saved, val Weighted F1: 0.5401)\n",
            "Fine-tune Epoch 10 Loss: 0.0116\n",
            "Fine-tune Epoch 11 Loss: 0.0097\n",
            "Fine-tune Epoch 12 Loss: 0.0078\n",
            "Fine-tune Epoch 13 Loss: 0.0070\n",
            "  (New best model saved, val Weighted F1: 0.5651)\n",
            "Fine-tune Epoch 14 Loss: 0.0059\n",
            "Fine-tune Epoch 15 Loss: 0.0058\n",
            "Fine-tune Epoch 16 Loss: 0.0051\n",
            "Fine-tune Epoch 17 Loss: 0.0039\n",
            "Fine-tune Epoch 18 Loss: 0.0039\n",
            "  (New best model saved, val Weighted F1: 0.5780)\n",
            "Fine-tune Epoch 19 Loss: 0.0036\n",
            "Fine-tune Epoch 20 Loss: 0.0033\n",
            "Fine-tune Epoch 21 Loss: 0.0033\n",
            "Fine-tune Epoch 22 Loss: 0.0032\n",
            "Fine-tune Epoch 23 Loss: 0.0036\n",
            "Fine-tune Epoch 24 Loss: 0.0033\n",
            "Fine-tune Epoch 25 Loss: 0.0029\n",
            "Fine-tune Epoch 26 Loss: 0.0025\n",
            "Fine-tune Epoch 27 Loss: 0.0027\n",
            "Fine-tune Epoch 28 Loss: 0.0021\n",
            "Fine-tune Epoch 29 Loss: 0.0015\n",
            "Fine-tune Epoch 30 Loss: 0.0017\n",
            "Fine-tune Epoch 31 Loss: 0.0018\n",
            "Fine-tune Epoch 32 Loss: 0.0025\n",
            "Fine-tune Epoch 33 Loss: 0.0019\n",
            "Fine-tune Epoch 34 Loss: 0.0016\n",
            "Fine-tune Epoch 35 Loss: 0.0014\n",
            "Fine-tune Epoch 36 Loss: 0.0009\n",
            "Fine-tune Epoch 37 Loss: 0.0009\n",
            "Fine-tune Epoch 38 Loss: 0.0007\n",
            "Fine-tune Epoch 39 Loss: 0.0007\n",
            "  (New best model saved, val Weighted F1: 0.5781)\n",
            "Fine-tune Epoch 40 Loss: 0.0006\n",
            "Fine-tune Epoch 41 Loss: 0.0005\n",
            "  (New best model saved, val Weighted F1: 0.5881)\n",
            "Fine-tune Epoch 42 Loss: 0.0006\n",
            "Fine-tune Epoch 43 Loss: 0.0005\n",
            "Fine-tune Epoch 44 Loss: 0.0004\n",
            "Fine-tune Epoch 45 Loss: 0.0005\n",
            "Fine-tune Epoch 46 Loss: 0.0005\n",
            "Fine-tune Epoch 47 Loss: 0.0006\n",
            "Fine-tune Epoch 48 Loss: 0.0007\n",
            "Fine-tune Epoch 49 Loss: 0.0007\n",
            "  (New best model saved, val Weighted F1: 0.5915)\n",
            "Fine-tune Epoch 50 Loss: 0.0003\n",
            "Fine-tune Epoch 51 Loss: 0.0004\n",
            "  (New best model saved, val Weighted F1: 0.5980)\n",
            "Fine-tune Epoch 52 Loss: 0.0006\n",
            "Fine-tune Epoch 53 Loss: 0.0007\n",
            "  (New best model saved, val Weighted F1: 0.6085)\n",
            "Fine-tune Epoch 54 Loss: 0.0005\n",
            "Fine-tune Epoch 55 Loss: 0.0004\n",
            "Fine-tune Epoch 56 Loss: 0.0004\n",
            "Fine-tune Epoch 57 Loss: 0.0004\n",
            "Fine-tune Epoch 58 Loss: 0.0004\n",
            "Fine-tune Epoch 59 Loss: 0.0003\n",
            "Fine-tune Epoch 60 Loss: 0.0004\n",
            "Fine-tune Epoch 61 Loss: 0.0003\n",
            "Fine-tune Epoch 62 Loss: 0.0004\n",
            "Fine-tune Epoch 63 Loss: 0.0003\n",
            "Fine-tune Epoch 64 Loss: 0.0003\n",
            "Fine-tune Epoch 65 Loss: 0.0003\n",
            "Fine-tune Epoch 66 Loss: 0.0002\n",
            "Fine-tune Epoch 67 Loss: 0.0002\n",
            "Fine-tune Epoch 68 Loss: 0.0002\n",
            "Fine-tune Epoch 69 Loss: 0.0003\n",
            "Fine-tune Epoch 70 Loss: 0.0003\n",
            "Fine-tune Epoch 71 Loss: 0.0003\n",
            "Fine-tune Epoch 72 Loss: 0.0001\n",
            "Fine-tune Epoch 73 Loss: 0.0002\n",
            "Fine-tune Epoch 74 Loss: 0.0001\n",
            "Fine-tune Epoch 75 Loss: 0.0001\n",
            "Fine-tune Epoch 76 Loss: 0.0002\n",
            "Fine-tune Epoch 77 Loss: 0.0001\n",
            "Fine-tune Epoch 78 Loss: 0.0002\n",
            "Fine-tune Epoch 79 Loss: 0.0002\n",
            "Fine-tune Epoch 80 Loss: 0.0001\n",
            "Fine-tune Epoch 81 Loss: 0.0002\n",
            "Fine-tune Epoch 82 Loss: 0.0001\n",
            "Fine-tune Epoch 83 Loss: 0.0001\n",
            "Fine-tune Epoch 84 Loss: 0.0001\n",
            "Fine-tune Epoch 85 Loss: 0.0001\n",
            "Fine-tune Epoch 86 Loss: 0.0003\n",
            "Fine-tune Epoch 87 Loss: 0.0002\n",
            "Fine-tune Epoch 88 Loss: 0.0002\n",
            "Fine-tune Epoch 89 Loss: 0.0002\n",
            "Fine-tune Epoch 90 Loss: 0.0001\n",
            "Fine-tune Epoch 91 Loss: 0.0001\n",
            "Fine-tune Epoch 92 Loss: 0.0002\n",
            "Fine-tune Epoch 93 Loss: 0.0001\n",
            "Fine-tune Epoch 94 Loss: 0.0001\n",
            "Fine-tune Epoch 95 Loss: 0.0001\n",
            "Fine-tune Epoch 96 Loss: 0.0002\n",
            "Fine-tune Epoch 97 Loss: 0.0001\n",
            "Fine-tune Epoch 98 Loss: 0.0001\n",
            "Fine-tune Epoch 99 Loss: 0.0001\n",
            "Fine-tune Epoch 100 Loss: 0.0001\n",
            "Evaluating best model with sweeping thresholds:\n",
            "Best F1/Threshold per class:\n",
            "Hip hop                             F1=0.739  th=0.40\n",
            "Rap                                 F1=0.744  th=0.58\n",
            "R&B / Soul                          F1=0.614  th=0.53\n",
            "Pop                                 F1=0.817  th=0.65\n",
            "Rock                                F1=0.731  th=0.45\n",
            "Jazz                                F1=0.080  th=0.13\n",
            "Blues                               F1=0.125  th=0.11\n",
            "Country                             F1=0.703  th=0.70\n",
            "Folk                                F1=0.326  th=0.21\n",
            "Classical                           F1=1.000  th=0.14\n",
            "Electronic                          F1=0.513  th=0.51\n",
            "Funk                                F1=0.000  th=0.50\n",
            "Reggae / Ska / Dub                  F1=0.000  th=0.50\n",
            "Metal                               F1=0.500  th=0.49\n",
            "Gospel / Religious                  F1=0.000  th=0.50\n",
            "Latin                               F1=0.000  th=0.50\n",
            "Disco / Dance                       F1=0.615  th=0.68\n",
            "Ambient / Chill / New Age           F1=0.435  th=0.14\n",
            "Experimental / Avant-Garde          F1=0.235  th=0.27\n",
            "Opera / Musical Theater / Soundtrack F1=0.000  th=0.50\n",
            "Vocal / A cappella                  F1=0.400  th=0.39\n",
            "Dancehall / Hip House / Club        F1=0.000  th=0.50\n",
            "Psychedelic                         F1=0.000  th=0.50\n",
            "Other / Niche                       F1=0.000  th=0.50\n",
            "\n",
            "Exact-match accuracy: 0.2500 (25.00%)\n",
            "Weighted F1: 0.6451\n",
            "Micro F1: 0.6282\n",
            "\n",
            "Classification report:\n",
            "\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                             Hip hop       0.77      0.71      0.74        48\n",
            "                                 Rap       0.82      0.68      0.74        47\n",
            "                          R&B / Soul       0.56      0.69      0.61        51\n",
            "                                 Pop       0.82      0.82      0.82       109\n",
            "                                Rock       0.71      0.76      0.73        45\n",
            "                                Jazz       0.05      0.20      0.08         5\n",
            "                               Blues       0.08      0.25      0.12         4\n",
            "                             Country       1.00      0.54      0.70        24\n",
            "                                Folk       0.22      0.64      0.33        11\n",
            "                           Classical       1.00      1.00      1.00         3\n",
            "                          Electronic       0.51      0.51      0.51        39\n",
            "                                Funk       0.00      0.00      0.00         0\n",
            "                  Reggae / Ska / Dub       0.00      0.00      0.00         1\n",
            "                               Metal       1.00      0.33      0.50         3\n",
            "                  Gospel / Religious       0.00      0.00      0.00         1\n",
            "                               Latin       0.00      0.00      0.00         0\n",
            "                       Disco / Dance       0.75      0.52      0.62        23\n",
            "           Ambient / Chill / New Age       0.32      0.67      0.43        15\n",
            "          Experimental / Avant-Garde       0.33      0.18      0.24        11\n",
            "Opera / Musical Theater / Soundtrack       0.00      0.00      0.00         2\n",
            "                  Vocal / A cappella       0.71      0.28      0.40        18\n",
            "        Dancehall / Hip House / Club       0.00      0.00      0.00         0\n",
            "                         Psychedelic       0.00      0.00      0.00         4\n",
            "                       Other / Niche       0.00      0.00      0.00         1\n",
            "\n",
            "                           micro avg       0.61      0.64      0.63       465\n",
            "                           macro avg       0.40      0.37      0.36       465\n",
            "                        weighted avg       0.68      0.64      0.65       465\n",
            "                         samples avg       0.66      0.68      0.63       465\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focal Loss: Helps model learn rare/hard labels and focus on \"hard\" examples.\n",
        "\n",
        "EfficientNet-B3: Highly expressive model, superior to VGG/AlexNet/ResNet18.\n",
        "\n",
        "Aggressive augmentations/training schedule: Maximizes potential on your dataset size.\n",
        "\n",
        "Threshold sweeping: Finds the best cut-off per label for true maximum F1/accuracy, not limited to fixed 0.5.\n",
        "\n",
        "Early stopping/checkpointing: Only keeps the best weights."
      ],
      "metadata": {
        "id": "FqDAW4uocrUP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CH_CvcQaSYPP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}