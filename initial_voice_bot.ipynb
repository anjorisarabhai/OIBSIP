{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLlJFl8brk+jh+46FuKjtB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjorisarabhai/OIBSIP/blob/main/initial_voice_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: SETUP AND INSTALLATION (Necessary libraries will be reinstalled) ---\n",
        "print(\"Installing necessary Python libraries...\")\n",
        "!pip install -q langchain pydantic langchain-core langchain-community requests\n",
        "\n",
        "# Install and start the Ollama server\n",
        "print(\"Setting up Ollama server...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh -s\n",
        "\n",
        "# --- Start Ollama server in the background and wait for readiness ---\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from datetime import datetime\n",
        "\n",
        "# Start 'ollama serve' in the background\n",
        "print(\"Starting Ollama service...\")\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "# Wait for Ollama server to be ready\n",
        "OLLAMA_URL = \"http://127.0.0.1:11434\"\n",
        "print(f\"Waiting for Ollama to be available at {OLLAMA_URL}...\")\n",
        "start_time = time.time()\n",
        "while time.time() - start_time < 60:\n",
        "    try:\n",
        "        requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
        "        print(\"Ollama server is ready!\")\n",
        "        break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        time.sleep(5)\n",
        "else:\n",
        "    raise Exception(\"Ollama server failed to start within the time limit. Check system logs.\")\n",
        "\n",
        "# Pull the Llama 3 8B model (no -q flag)\n",
        "print(\"Downloading Llama 3 model (8B)... This may take a few minutes.\")\n",
        "!ollama pull llama3\n",
        "\n",
        "# --- STEP 2: PYTHON CODE FOR DATA EXTRACTION (KEY FIX IMPLEMENTED) ---\n",
        "print(\"\\nDefining Pydantic Schema and Extraction Function...\")\n",
        "\n",
        "# --- A. Define the Pydantic Schema ---\n",
        "class VisitDetails(BaseModel):\n",
        "    \"\"\"Extracted details for scheduling a new visit in the CRM.\"\"\"\n",
        "    title: str = Field(description=\"A brief summary/title of the visit's purpose.\")\n",
        "    visit_type: Literal[\"OPERATION\", \"BUSINESS\", \"N/A\"] = Field(description=\"Must be one of the allowed Visit Types: OPERATION or BUSINESS.\")\n",
        "    lead_name: str = Field(description=\"The full name of the lead/client.\")\n",
        "    date: str = Field(description=\"The date of the visit in YYYY-MM-DD format.\")\n",
        "    start_time: str = Field(description=\"The exact start time in 24-hour format (HH:MM).\")\n",
        "    end_time: str = Field(description=\"The exact end time in 24-hour format (HH:MM). If not specified, set to 'N/A'.\")\n",
        "\n",
        "# --- B. Define the Extraction Function (FIXED) ---\n",
        "def extract_visit_data_final(transcript: str):\n",
        "    \"\"\"\n",
        "    Uses the Prompt + Parser method with explicit JSON instructions and safe templating.\n",
        "    \"\"\"\n",
        "    raw_output_string = \"\" # Initialize here to prevent UnboundLocalError\n",
        "    try:\n",
        "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        # 1. Initialize the Pydantic Parser\n",
        "        parser = PydanticOutputParser(pydantic_object=VisitDetails)\n",
        "\n",
        "        # 2. Get the specific formatting instructions from the parser\n",
        "        format_instructions = parser.get_format_instructions()\n",
        "\n",
        "        # 3. Define the System Prompt and Template (CRITICAL FIX: Use PromptTemplate and concatenate instructions safely)\n",
        "\n",
        "        # The prompt template only contains the variables we supply: transcript and custom instructions.\n",
        "        template = \"\"\"\n",
        "        SYSTEM INSTRUCTIONS: You are an expert CRM data extraction agent. Your task is to analyze the user's voice transcript and extract the required information into a strict JSON object.\n",
        "        Current Date: {current_date}. Infer dates and times relative to this date.\n",
        "        If any field is missing, set its value to 'N/A'.\n",
        "\n",
        "        {format_instructions}\n",
        "\n",
        "        USER TRANSCRIPT: {transcript}\n",
        "\n",
        "        RESPONSE:\n",
        "        \"\"\"\n",
        "\n",
        "        # Create the PromptTemplate\n",
        "        prompt = PromptTemplate(\n",
        "            template=template,\n",
        "            input_variables=[\"transcript\", \"current_date\", \"format_instructions\"]\n",
        "        )\n",
        "\n",
        "        # 4. Initialize the Llama 3 Model with JSON format flag\n",
        "        llm = ChatOllama(\n",
        "            model=\"llama3\",\n",
        "            base_url=OLLAMA_URL,\n",
        "            format=\"json\",\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        # 5. Create the Chain (Simple invoke)\n",
        "        chain = prompt | llm\n",
        "\n",
        "        # 6. Invoke the chain, passing the format_instructions separately\n",
        "        raw_output_message = chain.invoke({\n",
        "            \"transcript\": transcript,\n",
        "            \"current_date\": current_date,\n",
        "            \"format_instructions\": format_instructions # Pass the complex instructions here\n",
        "        })\n",
        "\n",
        "        # Get the content string\n",
        "        raw_output_string = raw_output_message.content\n",
        "\n",
        "        # 7. The parser attempts to convert the raw string into the Pydantic object\n",
        "        result = parser.parse(raw_output_string)\n",
        "\n",
        "        return result.dict()\n",
        "\n",
        "    except Exception as e:\n",
        "        # This will now safely print the raw output when a parsing error occurs\n",
        "        print(f\"\\n! RAW OUTPUT THAT CAUSED ERROR: \\n{raw_output_string}\\n\")\n",
        "        print(f\"An error occurred during final parsing. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- STEP 3: EXECUTION ---\n",
        "voice_input = \"Schedule a BUSINESS visit with Anjori Sarabhai for 7:40 PM tomorrow to discuss interest rates and follow-up on the loan status. The meeting should end 5 minutes later.\"\n",
        "\n",
        "print(\"\\n\\n--- RUNNING COMPATIBLE DATA EXTRACTION WITH LLAMA 3 ---\")\n",
        "extracted_data = extract_visit_data_final(voice_input)\n",
        "\n",
        "if extracted_data:\n",
        "    print(\"\\n Successfully Extracted Structured JSON:\")\n",
        "    print(json.dumps(extracted_data, indent=4))\n",
        "else:\n",
        "    print(\"\\n Failed to get structured data. See error details above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_w-bfa__Wwo",
        "outputId": "ce9c2b19-960f-4d3b-8742-1736c50b9345"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing necessary Python libraries...\n",
            "Setting up Ollama server...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Starting Ollama service...\n",
            "Waiting for Ollama to be available at http://127.0.0.1:11434...\n",
            "Ollama server is ready!\n",
            "Downloading Llama 3 model (8B)... This may take a few minutes.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\n",
            "Defining Pydantic Schema and Extraction Function...\n",
            "\n",
            "\n",
            "--- RUNNING COMPATIBLE DATA EXTRACTION WITH LLAMA 3 ---\n",
            "\n",
            " Successfully Extracted Structured JSON:\n",
            "{\n",
            "    \"title\": \"Follow-up on Loan Status\",\n",
            "    \"visit_type\": \"BUSINESS\",\n",
            "    \"lead_name\": \"Anjori Sarabhai\",\n",
            "    \"date\": \"2025-10-31\",\n",
            "    \"start_time\": \"19:40\",\n",
            "    \"end_time\": \"19:45\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-563480001.py:116: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return result.dict()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c-uc9OKV_0pu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}